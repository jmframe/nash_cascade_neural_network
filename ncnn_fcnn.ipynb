{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45cb35ce-803e-43be-b263-4a1a0f3bb6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nash Cascade Neural Network\n",
    "# A hydrologically intuitive deep learning network\n",
    "\n",
    "# Set up a solution to a network of buckets where the number of buckets in each layer\n",
    "# flows out to the buckets in the next layer\n",
    "# The parameter on each bucket is the size and height of each spigot.\n",
    "\n",
    "# Need a function that solves this individually at a single buckets\n",
    "# Then a function that loops through and moves the water to the downstream buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62bab940-cde5-4ff0-8897-75fdf2ebec52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from ncn_fcnn import NashCascadeNetwork as ncn\n",
    "from ncn_fcnn import train_model as train_ncnn\n",
    "import matplotlib.font_manager as font_manager\n",
    "# Precipitation standard variable name used in the ncnn model interface\n",
    "PRECIP_SVN = \"atmosphere_water__liquid_equivalent_precipitation_rate\"\n",
    "PRECIP_SVN_SEQ = \"atmosphere_water__liquid_equivalent_precipitation_rate_seq\"\n",
    "PRECIP_RECORD = \"atmosphere_water__liquid_equivalent_precipitation_rate_record\"\n",
    "DO_PLOT = True\n",
    "N_TIMESTEPS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46bcc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_precip_input_list = []\n",
    "count = 0\n",
    "unit_precip = 6.0\n",
    "for i in range(N_TIMESTEPS):\n",
    "\n",
    "    ###########################################################################\n",
    "    if count == 0:\n",
    "        network_precip_input_list.append(unit_precip)\n",
    "    elif count > 45:\n",
    "        network_precip_input_list.append(unit_precip)\n",
    "    else:\n",
    "        network_precip_input_list.append(np.random.random()*unit_precip/10)\n",
    "    if count == 50:\n",
    "        count = 0\n",
    "    count+=1\n",
    "    ###########################################################################\n",
    "network_precip_tensor = torch.tensor(network_precip_input_list, requires_grad=False)\n",
    "total_mass_precip_in = torch.sum(network_precip_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92cb4cbf-61b3-4566-a9e0-37d15e846fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Mass in network at start: 20.0\n",
      "Initial Mass in network: 20.0\n",
      "Final Mass in network: 28.3\n",
      "Total Mass out of network 251.3\n",
      "Total precipitation into network 259.5\n",
      "Mass balance for network is 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.4442, 0.4812, 0.2156, 0.3182, 0.4057])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 0\n",
    "bucket_net = ncn(cfg_file=\"./config_0.json\")\n",
    "bucket_net.initialize()\n",
    "bucket_net.unit_precip = unit_precip\n",
    "bucket_net.summarize_network()\n",
    "inital_mass_in_network = torch.sum(torch.tensor([tensor.item() for tensor in bucket_net.sum_H_per_layer]))\n",
    "print(f\"Initial Mass in network at start: {inital_mass_in_network:.1f}\")\n",
    "network_outflow_list_0 = []\n",
    "for i in range(N_TIMESTEPS):\n",
    "\n",
    "    ###########################################################################\n",
    "    ###########################################################################\n",
    "    bucket_net.set_value(PRECIP_SVN, torch.tensor(network_precip_input_list[i], requires_grad=False))\n",
    "    bucket_net.update()\n",
    "    network_outflow_list_0.append(bucket_net.network_outflow.item())\n",
    "    bucket_net.summarize_network()\n",
    "    ###########################################################################\n",
    "    ###########################################################################\n",
    "\n",
    "network_outflow_tensor_0 = torch.tensor(network_outflow_list_0, requires_grad=True)\n",
    "bucket_net.report_out_mass_balance()\n",
    "bucket_net.theta.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694044a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(thislinewillstopthenotebookfromgoinganyfurther)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "744fc71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Initial Mass in network at start: 20.0\n",
      "Initial Mass in network: 20.0\n",
      "Final Mass in network: 61.4\n",
      "Total Mass out of network 218.1\n",
      "Total precipitation into network 259.5\n",
      "Mass balance for network is -0.000\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "bucket_nn = ncn(cfg_file=\"./config_1_fcnn.json\")\n",
    "bucket_nn.initialize()\n",
    "bucket_nn.unit_precip = unit_precip\n",
    "print(\"Initialized\")\n",
    "bucket_nn.initialize_theta_values()\n",
    "inital_mass_in_network = torch.sum(torch.stack(bucket_nn.sum_H_per_layer)).item()\n",
    "print(f\"Initial Mass in network at start: {inital_mass_in_network:.1f}\")\n",
    "network_outflow_list_1a = []\n",
    "\n",
    "for i in range(N_TIMESTEPS):\n",
    "    # Set the current precipitation value\n",
    "    bucket_nn.set_value(PRECIP_SVN, torch.tensor(network_precip_input_list[i], requires_grad=True))\n",
    "\n",
    "    # Update the network (this should internally prepare and use the FCNN inputs)\n",
    "    bucket_nn.update()\n",
    "    network_outflow_list_1a.append(bucket_nn.network_outflow.item())\n",
    "    bucket_nn.summarize_network()\n",
    "    ###########################################################################\n",
    "    ###########################################################################\n",
    "\n",
    "###########################################################################\n",
    "network_outflow_tensor_1 = torch.tensor(network_outflow_list_1a, requires_grad=True)\n",
    "bucket_nn.report_out_mass_balance()\n",
    "\n",
    "origional_bucket_theta = copy.deepcopy(bucket_nn.theta.detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "217de549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL MODEL theta: tensor([0.5416, 0.4018, 0.5636, 0.5740, 0.6751], grad_fn=<SigmoidBackward0>)\n",
      "Adjusting learning rate of group 0 to 1.0000e-09.\n",
      "loss: 0.0480------------\n",
      "theta: tensor([0.5416, 0.4018, 0.5636, 0.5740, 0.6751], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.016024358570575714\n",
      "Adjusting learning rate of group 0 to 9.9000e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.4815, 0.4209, 0.6156, 0.5167, 0.6993], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 4.175687968687101e-18\n",
      "Adjusting learning rate of group 0 to 9.8010e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5667, 0.4109, 0.4604, 0.6459, 0.6453], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 9.7030e-10.\n",
      "loss: 1.0640------------\n",
      "theta: tensor([0.5391, 0.3966, 0.4969, 0.6144, 0.7180], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.014162610284984112\n",
      "Adjusting learning rate of group 0 to 9.6060e-10.\n",
      "loss: 0.2161------------\n",
      "theta: tensor([0.5427, 0.4152, 0.4739, 0.6266, 0.6707], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.010212174616754055\n",
      "Adjusting learning rate of group 0 to 9.5099e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.4814, 0.4210, 0.6156, 0.5167, 0.6993], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 8.083589477791975e-07\n",
      "Adjusting learning rate of group 0 to 9.4148e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5661, 0.4110, 0.4607, 0.6453, 0.6462], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 9.3207e-10.\n",
      "loss: 1.0931------------\n",
      "theta: tensor([0.4815, 0.4209, 0.6156, 0.5167, 0.6993], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.00019796770357061177\n",
      "Adjusting learning rate of group 0 to 9.2274e-10.\n",
      "loss: 0.1017------------\n",
      "theta: tensor([0.5707, 0.3991, 0.4978, 0.6284, 0.6564], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.015419971197843552\n",
      "Adjusting learning rate of group 0 to 9.1352e-10.\n",
      "loss: 1.0590------------\n",
      "theta: tensor([0.5332, 0.4188, 0.4768, 0.6199, 0.6762], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 9.0438e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5107, 0.4037, 0.5717, 0.5592, 0.7151], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 9.100900588698835e-11\n",
      "Adjusting learning rate of group 0 to 8.9534e-10.\n",
      "loss: 1.0932------------\n",
      "theta: tensor([0.4816, 0.4209, 0.6156, 0.5168, 0.6993], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 7.67592282500118e-05\n",
      "Adjusting learning rate of group 0 to 8.8638e-10.\n",
      "loss: 1.0932------------\n",
      "theta: tensor([0.5450, 0.4178, 0.4698, 0.6308, 0.6600], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 8.7752e-10.\n",
      "loss: 0.5629------------\n",
      "theta: tensor([0.5837, 0.3940, 0.5081, 0.6249, 0.6465], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.0018835869850590825\n",
      "Adjusting learning rate of group 0 to 8.6875e-10.\n",
      "loss: 0.0879------------\n",
      "theta: tensor([0.5685, 0.3976, 0.5148, 0.6141, 0.6605], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.007139871828258038\n",
      "Adjusting learning rate of group 0 to 8.6006e-10.\n",
      "loss: 0.0522------------\n",
      "theta: tensor([0.5054, 0.4109, 0.6017, 0.5352, 0.6939], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.03143234923481941\n",
      "Adjusting learning rate of group 0 to 8.5146e-10.\n",
      "loss: 1.0614------------\n",
      "theta: tensor([0.5756, 0.4042, 0.4639, 0.6535, 0.6434], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 8.4294e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5565, 0.4143, 0.4647, 0.6389, 0.6519], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 8.3451e-10.\n",
      "loss: 1.0828------------\n",
      "theta: tensor([0.5512, 0.4157, 0.4672, 0.6349, 0.6564], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.00022022043413016945\n",
      "Adjusting learning rate of group 0 to 8.2617e-10.\n",
      "loss: 0.1087------------\n",
      "theta: tensor([0.5463, 0.4017, 0.5466, 0.5863, 0.6741], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.01147384475916624\n",
      "Adjusting learning rate of group 0 to 8.1791e-10.\n",
      "loss: 0.5298------------\n",
      "theta: tensor([0.5837, 0.3942, 0.5027, 0.6279, 0.6481], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.0014018445508554578\n",
      "Adjusting learning rate of group 0 to 8.0973e-10.\n",
      "loss: 1.0718------------\n",
      "theta: tensor([0.5572, 0.4143, 0.4643, 0.6395, 0.6509], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 8.0163e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5591, 0.4135, 0.4636, 0.6407, 0.6502], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 7.9361e-10.\n",
      "loss: 1.0082------------\n",
      "theta: tensor([0.4827, 0.4203, 0.6147, 0.5178, 0.6994], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.02977665141224861\n",
      "Adjusting learning rate of group 0 to 7.8568e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5634, 0.4120, 0.4617, 0.6436, 0.6474], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 7.7782e-10.\n",
      "loss: 0.0334------------\n",
      "theta: tensor([0.5775, 0.3998, 0.4754, 0.6416, 0.6532], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.009642418473958969\n",
      "Adjusting learning rate of group 0 to 7.7004e-10.\n",
      "loss: 1.0919------------\n",
      "theta: tensor([0.4883, 0.4175, 0.6068, 0.5255, 0.7011], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 3.71229980089538e-08\n",
      "Adjusting learning rate of group 0 to 7.6234e-10.\n",
      "loss: 0.3444------------\n",
      "theta: tensor([0.5643, 0.4073, 0.4660, 0.6400, 0.6594], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.012343525886535645\n",
      "Adjusting learning rate of group 0 to 7.5472e-10.\n",
      "loss: 1.0932------------\n",
      "theta: tensor([0.5027, 0.4089, 0.5851, 0.5469, 0.7090], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 1.2948888109768492e-13\n",
      "Adjusting learning rate of group 0 to 7.4717e-10.\n",
      "loss: 0.5530------------\n",
      "theta: tensor([0.5764, 0.3950, 0.5215, 0.6138, 0.6511], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.004743458237498999\n",
      "Adjusting learning rate of group 0 to 7.3970e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5664, 0.4109, 0.4605, 0.6456, 0.6458], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 7.3230e-10.\n",
      "loss: 0.0657------------\n",
      "theta: tensor([0.5798, 0.3980, 0.4766, 0.6409, 0.6547], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.013673825189471245\n",
      "Adjusting learning rate of group 0 to 7.2498e-10.\n",
      "loss: 0.5636------------\n",
      "theta: tensor([0.5323, 0.4115, 0.4854, 0.6149, 0.6947], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.05854514613747597\n",
      "Adjusting learning rate of group 0 to 7.1773e-10.\n",
      "loss: 0.6178------------\n",
      "theta: tensor([0.5837, 0.3935, 0.5137, 0.6216, 0.6456], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.0015879805432632565\n",
      "Adjusting learning rate of group 0 to 7.1055e-10.\n",
      "loss: 0.3555------------\n",
      "theta: tensor([0.5697, 0.4064, 0.4634, 0.6445, 0.6529], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.015051043592393398\n",
      "Adjusting learning rate of group 0 to 7.0345e-10.\n",
      "loss: 1.0656------------\n",
      "theta: tensor([0.5662, 0.4109, 0.4607, 0.6454, 0.6461], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 6.9641e-10.\n",
      "loss: 0.1083------------\n",
      "theta: tensor([0.5682, 0.3998, 0.4906, 0.6281, 0.6642], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.024507736787199974\n",
      "Adjusting learning rate of group 0 to 6.8945e-10.\n",
      "loss: 0.0535------------\n",
      "theta: tensor([0.5798, 0.3948, 0.5118, 0.6211, 0.6494], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.006044415291398764\n",
      "Adjusting learning rate of group 0 to 6.8255e-10.\n",
      "loss: 1.0547------------\n",
      "theta: tensor([0.5775, 0.4022, 0.4645, 0.6546, 0.6451], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.00010309964272892103\n",
      "Adjusting learning rate of group 0 to 6.7573e-10.\n",
      "loss: 0.0731------------\n",
      "theta: tensor([0.5790, 0.4011, 0.4631, 0.6480, 0.6523], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.0036308893468230963\n",
      "Adjusting learning rate of group 0 to 6.6897e-10.\n",
      "H_final is suspiciously low tensor([-1.2011e-01,  1.0916e+01,  2.3839e+02,  3.6979e-01],\n",
      "       grad_fn=<SubBackward0>)\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5628, 0.4122, 0.4620, 0.6432, 0.6480], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 6.6228e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5658, 0.4110, 0.4609, 0.6451, 0.6465], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 6.5566e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5664, 0.4108, 0.4606, 0.6455, 0.6460], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 6.4910e-10.\n",
      "loss: 1.0930------------\n",
      "theta: tensor([0.4814, 0.4210, 0.6156, 0.5167, 0.6993], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.00033558261930011213\n",
      "Adjusting learning rate of group 0 to 6.4261e-10.\n",
      "loss: 1.0826------------\n",
      "theta: tensor([0.5778, 0.4023, 0.4642, 0.6549, 0.6443], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 6.3619e-10.\n",
      "loss: 1.0542------------\n",
      "theta: tensor([0.5665, 0.4136, 0.4597, 0.6490, 0.6356], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 6.2982e-10.\n",
      "loss: 0.2613------------\n",
      "theta: tensor([0.5057, 0.4095, 0.5799, 0.5505, 0.7051], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.05969979986548424\n",
      "Adjusting learning rate of group 0 to 6.2353e-10.\n",
      "loss: 1.0929------------\n",
      "theta: tensor([0.4814, 0.4210, 0.6156, 0.5167, 0.6993], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.0004433915310073644\n",
      "Adjusting learning rate of group 0 to 6.1729e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5487, 0.4167, 0.4682, 0.6333, 0.6576], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 6.1112e-10.\n",
      "loss: 1.0930------------\n",
      "theta: tensor([0.4920, 0.4143, 0.6006, 0.5319, 0.7058], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.000346417014952749\n",
      "Adjusting learning rate of group 0 to 6.0501e-10.\n",
      "loss: 1.0516------------\n",
      "theta: tensor([0.5667, 0.4112, 0.4602, 0.6461, 0.6445], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 9.758827218320221e-05\n",
      "Adjusting learning rate of group 0 to 5.9896e-10.\n",
      "loss: 0.0344------------\n",
      "theta: tensor([0.5683, 0.3996, 0.4931, 0.6273, 0.6632], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.012629326432943344\n",
      "Adjusting learning rate of group 0 to 5.9297e-10.\n",
      "loss: 0.2929------------\n",
      "theta: tensor([0.5703, 0.4055, 0.4633, 0.6443, 0.6547], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.010081253945827484\n",
      "Adjusting learning rate of group 0 to 5.8704e-10.\n",
      "loss: 0.0456------------\n",
      "theta: tensor([0.5498, 0.4019, 0.5227, 0.6024, 0.6768], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.022366564720869064\n",
      "Adjusting learning rate of group 0 to 5.8117e-10.\n",
      "loss: 1.0915------------\n",
      "theta: tensor([0.4942, 0.4129, 0.5975, 0.5350, 0.7071], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.0021707855630666018\n",
      "Adjusting learning rate of group 0 to 5.7535e-10.\n",
      "loss: 0.0732------------\n",
      "theta: tensor([0.5809, 0.3952, 0.5037, 0.6266, 0.6494], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.005042924080044031\n",
      "Adjusting learning rate of group 0 to 5.6960e-10.\n",
      "loss: 0.0273------------\n",
      "theta: tensor([0.5330, 0.4046, 0.5586, 0.5732, 0.6836], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.02678126096725464\n",
      "Adjusting learning rate of group 0 to 5.6391e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.4825, 0.4203, 0.6139, 0.5183, 0.7000], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 3.6444859796314703e-17\n",
      "Adjusting learning rate of group 0 to 5.5827e-10.\n",
      "loss: 1.0905------------\n",
      "theta: tensor([0.5628, 0.4115, 0.4625, 0.6426, 0.6499], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 5.242240513325669e-05\n",
      "Adjusting learning rate of group 0 to 5.5268e-10.\n",
      "loss: 1.0850------------\n",
      "theta: tensor([0.5771, 0.4025, 0.4645, 0.6543, 0.6450], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 5.4716e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.4816, 0.4209, 0.6156, 0.5168, 0.6993], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 3.9256349918250776e-18\n",
      "Adjusting learning rate of group 0 to 5.4169e-10.\n",
      "loss: 0.2079------------\n",
      "theta: tensor([0.5735, 0.4044, 0.4629, 0.6462, 0.6523], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.009905032813549042\n",
      "Adjusting learning rate of group 0 to 5.3627e-10.\n",
      "loss: 0.0567------------\n",
      "theta: tensor([0.4936, 0.4155, 0.6083, 0.5263, 0.6978], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.03568492457270622\n",
      "Adjusting learning rate of group 0 to 5.3091e-10.\n",
      "loss: 0.0885------------\n",
      "theta: tensor([0.5567, 0.4084, 0.4737, 0.6336, 0.6656], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.01107060443609953\n",
      "Adjusting learning rate of group 0 to 5.2560e-10.\n",
      "loss: 0.0923------------\n",
      "theta: tensor([0.5749, 0.3972, 0.5025, 0.6255, 0.6542], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.010061003267765045\n",
      "Adjusting learning rate of group 0 to 5.2034e-10.\n",
      "loss: 0.3630------------\n",
      "theta: tensor([0.5438, 0.4123, 0.4771, 0.6250, 0.6762], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.028049275279045105\n",
      "Adjusting learning rate of group 0 to 5.1514e-10.\n",
      "loss: 0.1619------------\n",
      "theta: tensor([0.5601, 0.3987, 0.5338, 0.5995, 0.6642], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.00797959603369236\n",
      "Adjusting learning rate of group 0 to 5.0999e-10.\n",
      "loss: 0.5186------------\n",
      "theta: tensor([0.5740, 0.3958, 0.5194, 0.6139, 0.6541], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.0029932204633951187\n",
      "Adjusting learning rate of group 0 to 5.0489e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5321, 0.4189, 0.4775, 0.6189, 0.6778], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 4.9984e-10.\n",
      "loss: 0.1873------------\n",
      "theta: tensor([0.4982, 0.4130, 0.5928, 0.5389, 0.7027], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.03779696300625801\n",
      "Adjusting learning rate of group 0 to 4.9484e-10.\n",
      "loss: 0.9396------------\n",
      "theta: tensor([0.5410, 0.4130, 0.4774, 0.6225, 0.6796], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.05227944254875183\n",
      "Adjusting learning rate of group 0 to 4.8989e-10.\n",
      "loss: 1.0926------------\n",
      "theta: tensor([0.5764, 0.3942, 0.4758, 0.6488, 0.6654], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 2.8765714887413196e-05\n",
      "Adjusting learning rate of group 0 to 4.8499e-10.\n",
      "loss: 1.0910------------\n",
      "theta: tensor([0.5333, 0.4188, 0.4767, 0.6200, 0.6760], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 4.8014e-10.\n",
      "loss: 0.0363------------\n",
      "theta: tensor([0.5509, 0.3999, 0.5537, 0.5841, 0.6685], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.015433646738529205\n",
      "Adjusting learning rate of group 0 to 4.7534e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.4836, 0.4196, 0.6123, 0.5199, 0.7007], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 8.486273279588289e-16\n",
      "Adjusting learning rate of group 0 to 4.7059e-10.\n",
      "loss: 0.0322------------\n",
      "theta: tensor([0.5698, 0.3984, 0.5012, 0.6233, 0.6607], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.007269352208822966\n",
      "Adjusting learning rate of group 0 to 4.6588e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.4817, 0.4208, 0.6155, 0.5169, 0.6993], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 5.038902600063011e-06\n",
      "Adjusting learning rate of group 0 to 4.6122e-10.\n",
      "loss: 0.0890------------\n",
      "theta: tensor([0.5270, 0.4048, 0.5813, 0.5570, 0.6838], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.020732533186674118\n",
      "Adjusting learning rate of group 0 to 4.5661e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5662, 0.4111, 0.4606, 0.6455, 0.6455], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 4.5204e-10.\n",
      "loss: 1.0699------------\n",
      "theta: tensor([0.5699, 0.4080, 0.4629, 0.6504, 0.6434], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.00045452569611370564\n",
      "Adjusting learning rate of group 0 to 4.4752e-10.\n",
      "loss: 0.0901------------\n",
      "theta: tensor([0.5666, 0.4016, 0.4821, 0.6323, 0.6650], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.02234829030930996\n",
      "Adjusting learning rate of group 0 to 4.4305e-10.\n",
      "loss: 1.0777------------\n",
      "theta: tensor([0.5628, 0.4120, 0.4622, 0.6430, 0.6485], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.00029252705280669034\n",
      "Adjusting learning rate of group 0 to 4.3862e-10.\n",
      "loss: 0.0574------------\n",
      "theta: tensor([0.5545, 0.3992, 0.5487, 0.5884, 0.6664], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.013256646692752838\n",
      "Adjusting learning rate of group 0 to 4.3423e-10.\n",
      "loss: 0.6113------------\n",
      "theta: tensor([0.5372, 0.4072, 0.4950, 0.6120, 0.6945], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.0806555226445198\n",
      "Adjusting learning rate of group 0 to 4.2989e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.4818, 0.4208, 0.6155, 0.5169, 0.6993], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 2.212209580280551e-16\n",
      "Adjusting learning rate of group 0 to 4.2559e-10.\n",
      "loss: 0.0487------------\n",
      "theta: tensor([0.5292, 0.4048, 0.5734, 0.5629, 0.6830], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.016356227919459343\n",
      "Adjusting learning rate of group 0 to 4.2133e-10.\n",
      "loss: 0.0589------------\n",
      "theta: tensor([0.5710, 0.3973, 0.5129, 0.6172, 0.6570], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.008246085606515408\n",
      "Adjusting learning rate of group 0 to 4.1712e-10.\n",
      "loss: 0.1761------------\n",
      "theta: tensor([0.5479, 0.4044, 0.4957, 0.6174, 0.6835], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.043858665972948074\n",
      "Adjusting learning rate of group 0 to 4.1295e-10.\n",
      "loss: 1.0932------------\n",
      "theta: tensor([0.5663, 0.4109, 0.4606, 0.6455, 0.6460], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 4.0882e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5318, 0.4189, 0.4777, 0.6186, 0.6782], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 4.0473e-10.\n",
      "loss: 0.2696------------\n",
      "theta: tensor([0.5811, 0.3954, 0.5039, 0.6272, 0.6480], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.0019419671734794974\n",
      "Adjusting learning rate of group 0 to 4.0068e-10.\n",
      "loss: 0.0405------------\n",
      "theta: tensor([0.5721, 0.3980, 0.4984, 0.6257, 0.6592], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.008494076319038868\n",
      "Adjusting learning rate of group 0 to 3.9668e-10.\n",
      "loss: 1.0932------------\n",
      "theta: tensor([0.5319, 0.4075, 0.4895, 0.6129, 0.7045], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 1.1438573178923495e-11\n",
      "Adjusting learning rate of group 0 to 3.9271e-10.\n",
      "loss: 1.0616------------\n",
      "theta: tensor([0.5262, 0.4161, 0.4838, 0.6118, 0.6937], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.010003721341490746\n",
      "Adjusting learning rate of group 0 to 3.8878e-10.\n",
      "loss: 0.1053------------\n",
      "theta: tensor([0.5308, 0.4050, 0.5569, 0.5749, 0.6856], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.04324460029602051\n",
      "Adjusting learning rate of group 0 to 3.8490e-10.\n",
      "loss: 0.0570------------\n",
      "theta: tensor([0.5559, 0.4009, 0.5145, 0.6095, 0.6724], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.011688518337905407\n",
      "Adjusting learning rate of group 0 to 3.8105e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.4816, 0.4209, 0.6155, 0.5168, 0.6993], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 1.547758323663284e-20\n",
      "Adjusting learning rate of group 0 to 3.7724e-10.\n",
      "loss: 1.0751------------\n",
      "theta: tensor([0.5644, 0.4113, 0.4616, 0.6440, 0.6480], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.00032556994119659066\n",
      "Adjusting learning rate of group 0 to 3.7346e-10.\n",
      "loss: 0.4128------------\n",
      "theta: tensor([0.5457, 0.4003, 0.5297, 0.6010, 0.6823], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.06550944596529007\n",
      "Adjusting learning rate of group 0 to 3.6973e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.4815, 0.4209, 0.6156, 0.5167, 0.6993], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 1.0793776983319958e-09\n",
      "Adjusting learning rate of group 0 to 3.6603e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5297, 0.4189, 0.4790, 0.6165, 0.6814], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 2.439503532514209e-07\n",
      "Adjusting learning rate of group 0 to 3.6237e-10.\n",
      "loss: 1.0494------------\n",
      "theta: tensor([0.5313, 0.4189, 0.4780, 0.6181, 0.6790], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.0008018120424821973\n",
      "Adjusting learning rate of group 0 to 3.5875e-10.\n",
      "loss: 1.0516------------\n",
      "theta: tensor([0.5658, 0.4110, 0.4609, 0.6451, 0.6466], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 3.5516e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5668, 0.4108, 0.4604, 0.6459, 0.6455], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 3.5161e-10.\n",
      "loss: 0.0535------------\n",
      "theta: tensor([0.5647, 0.4044, 0.4757, 0.6361, 0.6621], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.0067220572382211685\n",
      "Adjusting learning rate of group 0 to 3.4809e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5640, 0.4147, 0.4600, 0.6468, 0.6376], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 3.4461e-10.\n",
      "loss: 1.0920------------\n",
      "theta: tensor([0.5108, 0.4056, 0.5677, 0.5609, 0.7117], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 1.0301199182549681e-07\n",
      "Adjusting learning rate of group 0 to 3.4117e-10.\n",
      "loss: 0.6422------------\n",
      "theta: tensor([0.5364, 0.4005, 0.4986, 0.6115, 0.7116], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.09971737861633301\n",
      "Adjusting learning rate of group 0 to 3.3775e-10.\n",
      "loss: 1.0140------------\n",
      "theta: tensor([0.5479, 0.4127, 0.4720, 0.6289, 0.6705], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.022661777213215828\n",
      "Adjusting learning rate of group 0 to 3.3438e-10.\n",
      "loss: 1.0875------------\n",
      "theta: tensor([0.4835, 0.4199, 0.6131, 0.5194, 0.6998], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.009399049915373325\n",
      "Adjusting learning rate of group 0 to 3.3103e-10.\n",
      "loss: 1.0893------------\n",
      "theta: tensor([0.5553, 0.4142, 0.4656, 0.6376, 0.6541], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 8.122579311020672e-05\n",
      "Adjusting learning rate of group 0 to 3.2772e-10.\n",
      "loss: 0.0268------------\n",
      "theta: tensor([0.5349, 0.4041, 0.5590, 0.5742, 0.6814], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.021080538630485535\n",
      "Adjusting learning rate of group 0 to 3.2445e-10.\n",
      "loss: 0.0954------------\n",
      "theta: tensor([0.5787, 0.3954, 0.5071, 0.6233, 0.6516], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.003907674457877874\n",
      "Adjusting learning rate of group 0 to 3.2120e-10.\n",
      "loss: 0.0686------------\n",
      "theta: tensor([0.5280, 0.4055, 0.5689, 0.5652, 0.6850], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.01780518889427185\n",
      "Adjusting learning rate of group 0 to 3.1799e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.4819, 0.4207, 0.6154, 0.5170, 0.6993], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 4.600616648531286e-06\n",
      "Adjusting learning rate of group 0 to 3.1481e-10.\n",
      "loss: 0.5683------------\n",
      "theta: tensor([0.5759, 0.3953, 0.5206, 0.6141, 0.6518], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.004971843678504229\n",
      "Adjusting learning rate of group 0 to 3.1166e-10.\n",
      "loss: 0.1917------------\n",
      "theta: tensor([0.5023, 0.4125, 0.5981, 0.5365, 0.6960], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.07403859496116638\n",
      "Adjusting learning rate of group 0 to 3.0854e-10.\n",
      "loss: 0.0394------------\n",
      "theta: tensor([0.5543, 0.4008, 0.5243, 0.6024, 0.6722], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.012556168250739574\n",
      "Adjusting learning rate of group 0 to 3.0546e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.4815, 0.4209, 0.6153, 0.5170, 0.6994], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 9.149912078010303e-11\n",
      "Adjusting learning rate of group 0 to 3.0240e-10.\n",
      "loss: 0.1903------------\n",
      "theta: tensor([0.5847, 0.3954, 0.4816, 0.6399, 0.6516], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.002956849755719304\n",
      "Adjusting learning rate of group 0 to 2.9938e-10.\n",
      "loss: 0.2337------------\n",
      "theta: tensor([0.4876, 0.4179, 0.6135, 0.5204, 0.6988], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.051807306706905365\n",
      "Adjusting learning rate of group 0 to 2.9639e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5496, 0.4165, 0.4677, 0.6340, 0.6567], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 2.9342e-10.\n",
      "loss: 0.0504------------\n",
      "theta: tensor([0.5519, 0.3997, 0.5514, 0.5858, 0.6681], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.018513189628720284\n",
      "Adjusting learning rate of group 0 to 2.9049e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5636, 0.4118, 0.4618, 0.6436, 0.6477], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 2.8758e-10.\n",
      "loss: 0.4218------------\n",
      "theta: tensor([0.5187, 0.4077, 0.5632, 0.5650, 0.6963], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.09477157890796661\n",
      "Adjusting learning rate of group 0 to 2.8471e-10.\n",
      "loss: 0.1006------------\n",
      "theta: tensor([0.5579, 0.4002, 0.5203, 0.6070, 0.6689], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.02868378348648548\n",
      "Adjusting learning rate of group 0 to 2.8186e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5623, 0.4128, 0.4620, 0.6432, 0.6468], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 2.7904e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5375, 0.3890, 0.5268, 0.5993, 0.7280], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 1.105703636289368e-12\n",
      "Adjusting learning rate of group 0 to 2.7625e-10.\n",
      "loss: 1.0891------------\n",
      "theta: tensor([0.5543, 0.4168, 0.4646, 0.6392, 0.6476], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.00012744733248837292\n",
      "Adjusting learning rate of group 0 to 2.7349e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5666, 0.4108, 0.4605, 0.6457, 0.6457], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 2.7075e-10.\n",
      "loss: 0.6741------------\n",
      "theta: tensor([0.5826, 0.3943, 0.5092, 0.6241, 0.6467], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.0015997806331142783\n",
      "Adjusting learning rate of group 0 to 2.6805e-10.\n",
      "loss: 0.3098------------\n",
      "theta: tensor([0.5506, 0.4001, 0.5241, 0.6063, 0.6767], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.050286486744880676\n",
      "Adjusting learning rate of group 0 to 2.6537e-10.\n",
      "loss: 0.0650------------\n",
      "theta: tensor([0.5776, 0.4019, 0.4638, 0.6473, 0.6521], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.008295826613903046\n",
      "Adjusting learning rate of group 0 to 2.6271e-10.\n",
      "loss: 1.0911------------\n",
      "theta: tensor([0.5339, 0.4188, 0.4764, 0.6206, 0.6751], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 2.6009e-10.\n",
      "loss: 1.0932------------\n",
      "theta: tensor([0.5283, 0.4131, 0.4851, 0.6127, 0.6975], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 1.2426815132471347e-09\n",
      "Adjusting learning rate of group 0 to 2.5748e-10.\n",
      "loss: 0.8710------------\n",
      "theta: tensor([0.5341, 0.4184, 0.4766, 0.6205, 0.6760], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.017134839668869972\n",
      "Adjusting learning rate of group 0 to 2.5491e-10.\n",
      "loss: 0.0254------------\n",
      "theta: tensor([0.5679, 0.3980, 0.5117, 0.6159, 0.6614], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.012291686609387398\n",
      "Adjusting learning rate of group 0 to 2.5236e-10.\n",
      "loss: 0.8044------------\n",
      "theta: tensor([0.5791, 0.3945, 0.5175, 0.6173, 0.6494], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.00014790754357818514\n",
      "Adjusting learning rate of group 0 to 2.4984e-10.\n",
      "loss: 1.0900------------\n",
      "theta: tensor([0.5294, 0.4190, 0.4792, 0.6163, 0.6818], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 9.195964230457321e-05\n",
      "Adjusting learning rate of group 0 to 2.4734e-10.\n",
      "loss: 0.0543------------\n",
      "theta: tensor([0.5137, 0.4086, 0.5729, 0.5576, 0.6978], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.0394592322409153\n",
      "Adjusting learning rate of group 0 to 2.4487e-10.\n",
      "loss: 1.0511------------\n",
      "theta: tensor([0.5313, 0.4189, 0.4780, 0.6181, 0.6790], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.0008706859080120921\n",
      "Adjusting learning rate of group 0 to 2.4242e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.4816, 0.4209, 0.6156, 0.5168, 0.6993], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 7.852449336408503e-19\n",
      "Adjusting learning rate of group 0 to 2.3999e-10.\n",
      "loss: 1.0394------------\n",
      "theta: tensor([0.5193, 0.4043, 0.5477, 0.5759, 0.7099], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.2617499530315399\n",
      "Adjusting learning rate of group 0 to 2.3759e-10.\n",
      "loss: 0.0613------------\n",
      "theta: tensor([0.5522, 0.4009, 0.5338, 0.5965, 0.6714], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.03512116149067879\n",
      "Adjusting learning rate of group 0 to 2.3522e-10.\n",
      "loss: 0.1417------------\n",
      "theta: tensor([0.5790, 0.3979, 0.4809, 0.6387, 0.6540], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.015459537506103516\n",
      "Adjusting learning rate of group 0 to 2.3286e-10.\n",
      "loss: 0.0483------------\n",
      "theta: tensor([0.5417, 0.4030, 0.5502, 0.5844, 0.6752], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.02485567517578602\n",
      "Adjusting learning rate of group 0 to 2.3054e-10.\n",
      "loss: 0.0687------------\n",
      "theta: tensor([0.5719, 0.4015, 0.4771, 0.6385, 0.6574], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.007401215843856335\n",
      "Adjusting learning rate of group 0 to 2.2823e-10.\n",
      "loss: 0.0345------------\n",
      "theta: tensor([0.5790, 0.3972, 0.4863, 0.6356, 0.6541], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.010141821578145027\n",
      "Adjusting learning rate of group 0 to 2.2595e-10.\n",
      "loss: 1.0925------------\n",
      "theta: tensor([0.5337, 0.4188, 0.4765, 0.6204, 0.6755], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 2.2369e-10.\n",
      "loss: 0.4359------------\n",
      "theta: tensor([0.5452, 0.4144, 0.4733, 0.6284, 0.6688], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.021571263670921326\n",
      "Adjusting learning rate of group 0 to 2.2145e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5324, 0.4189, 0.4773, 0.6192, 0.6773], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 2.1924e-10.\n",
      "loss: 0.0307------------\n",
      "theta: tensor([0.5843, 0.3946, 0.4958, 0.6322, 0.6485], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.004677123855799437\n",
      "Adjusting learning rate of group 0 to 2.1704e-10.\n",
      "loss: 0.0738------------\n",
      "theta: tensor([0.5792, 0.3958, 0.4987, 0.6282, 0.6532], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.005253974813967943\n",
      "Adjusting learning rate of group 0 to 2.1487e-10.\n",
      "loss: 0.0700------------\n",
      "theta: tensor([0.5717, 0.4004, 0.4812, 0.6357, 0.6594], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.003153237048536539\n",
      "Adjusting learning rate of group 0 to 2.1273e-10.\n",
      "loss: 0.7738------------\n",
      "theta: tensor([0.4826, 0.4204, 0.6152, 0.5174, 0.6993], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.07785257697105408\n",
      "Adjusting learning rate of group 0 to 2.1060e-10.\n",
      "loss: 0.1836------------\n",
      "theta: tensor([0.5373, 0.4028, 0.5666, 0.5702, 0.6783], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.010840844362974167\n",
      "Adjusting learning rate of group 0 to 2.0849e-10.\n",
      "loss: 0.0806------------\n",
      "theta: tensor([0.5553, 0.4046, 0.4863, 0.6259, 0.6740], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.017998887225985527\n",
      "Adjusting learning rate of group 0 to 2.0641e-10.\n",
      "loss: 1.0932------------\n",
      "theta: tensor([0.5295, 0.4189, 0.4792, 0.6163, 0.6817], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 1.8684479528019438e-06\n",
      "Adjusting learning rate of group 0 to 2.0434e-10.\n",
      "loss: 1.0371------------\n",
      "theta: tensor([0.5526, 0.4155, 0.4665, 0.6361, 0.6548], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.0003607045509852469\n",
      "Adjusting learning rate of group 0 to 2.0230e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.4815, 0.4209, 0.6156, 0.5167, 0.6993], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 5.3228095894678265e-14\n",
      "Adjusting learning rate of group 0 to 2.0028e-10.\n",
      "loss: 0.0950------------\n",
      "theta: tensor([0.5691, 0.4036, 0.4693, 0.6402, 0.6604], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.012969842180609703\n",
      "Adjusting learning rate of group 0 to 1.9827e-10.\n",
      "loss: 1.0866------------\n",
      "theta: tensor([0.5762, 0.3939, 0.4767, 0.6483, 0.6664], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.0002655439602676779\n",
      "Adjusting learning rate of group 0 to 1.9629e-10.\n",
      "loss: 1.0670------------\n",
      "theta: tensor([0.5325, 0.4189, 0.4773, 0.6192, 0.6772], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 1.9433e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.4990, 0.4101, 0.5905, 0.5418, 0.7096], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 1.3920536501871197e-16\n",
      "Adjusting learning rate of group 0 to 1.9239e-10.\n",
      "loss: 0.4774------------\n",
      "theta: tensor([0.5327, 0.4149, 0.4811, 0.6172, 0.6866], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.03735213726758957\n",
      "Adjusting learning rate of group 0 to 1.9046e-10.\n",
      "loss: 0.8339------------\n",
      "theta: tensor([0.5309, 0.4184, 0.4787, 0.6174, 0.6809], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.019869612529873848\n",
      "Adjusting learning rate of group 0 to 1.8856e-10.\n",
      "loss: 0.0374------------\n",
      "theta: tensor([0.5690, 0.3984, 0.5013, 0.6221, 0.6627], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.009575042873620987\n",
      "Adjusting learning rate of group 0 to 1.8667e-10.\n",
      "loss: 0.0927------------\n",
      "theta: tensor([0.5766, 0.3965, 0.5048, 0.6247, 0.6523], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.014062019065022469\n",
      "Adjusting learning rate of group 0 to 1.8480e-10.\n",
      "loss: 0.1897------------\n",
      "theta: tensor([0.5512, 0.4094, 0.4766, 0.6294, 0.6713], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.022352417930960655\n",
      "Adjusting learning rate of group 0 to 1.8296e-10.\n",
      "loss: 1.0853------------\n",
      "theta: tensor([0.5629, 0.4116, 0.4624, 0.6428, 0.6495], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.00015499995788559318\n",
      "Adjusting learning rate of group 0 to 1.8113e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5511, 0.4160, 0.4670, 0.6351, 0.6556], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 1.7932e-10.\n",
      "loss: 0.0449------------\n",
      "theta: tensor([0.5669, 0.4012, 0.4856, 0.6312, 0.6639], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.006552604958415031\n",
      "Adjusting learning rate of group 0 to 1.7752e-10.\n",
      "loss: 0.0339------------\n",
      "theta: tensor([0.5781, 0.4008, 0.4665, 0.6459, 0.6535], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.007722759619355202\n",
      "Adjusting learning rate of group 0 to 1.7575e-10.\n",
      "loss: 0.1733------------\n",
      "theta: tensor([0.5738, 0.3986, 0.4939, 0.6308, 0.6550], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.015445513650774956\n",
      "Adjusting learning rate of group 0 to 1.7399e-10.\n",
      "loss: 1.0312------------\n",
      "theta: tensor([0.5287, 0.4186, 0.4800, 0.6153, 0.6839], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.005911064334213734\n",
      "Adjusting learning rate of group 0 to 1.7225e-10.\n",
      "loss: 0.0396------------\n",
      "theta: tensor([0.5853, 0.3982, 0.4642, 0.6502, 0.6492], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.002978413598611951\n",
      "Adjusting learning rate of group 0 to 1.7053e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5633, 0.4140, 0.4606, 0.6454, 0.6411], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 1.6882e-10.\n",
      "loss: 1.0815------------\n",
      "theta: tensor([0.5302, 0.4189, 0.4787, 0.6170, 0.6807], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.00032117709633894265\n",
      "Adjusting learning rate of group 0 to 1.6713e-10.\n",
      "loss: 1.0881------------\n",
      "theta: tensor([0.5603, 0.4127, 0.4634, 0.6411, 0.6507], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.00010858269524760544\n",
      "Adjusting learning rate of group 0 to 1.6546e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.5317, 0.4189, 0.4777, 0.6185, 0.6783], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 1.6381e-10.\n",
      "loss: 0.0326------------\n",
      "theta: tensor([0.5793, 0.3968, 0.4936, 0.6325, 0.6511], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.005535287316888571\n",
      "Adjusting learning rate of group 0 to 1.6217e-10.\n",
      "loss: 0.1641------------\n",
      "theta: tensor([0.5655, 0.4042, 0.4770, 0.6380, 0.6590], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.00878859031945467\n",
      "Adjusting learning rate of group 0 to 1.6055e-10.\n",
      "loss: 1.0914------------\n",
      "theta: tensor([0.5447, 0.4172, 0.4704, 0.6299, 0.6624], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 4.093229290447198e-05\n",
      "Adjusting learning rate of group 0 to 1.5894e-10.\n",
      "loss: 1.0933------------\n",
      "theta: tensor([0.4825, 0.4204, 0.6148, 0.5176, 0.6994], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 1.1842211781500078e-14\n",
      "Adjusting learning rate of group 0 to 1.5735e-10.\n",
      "loss: 1.0588------------\n",
      "theta: tensor([0.5387, 0.4184, 0.4735, 0.6250, 0.6686], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 1.5578e-10.\n",
      "loss: 1.0557------------\n",
      "theta: tensor([0.5323, 0.4189, 0.4774, 0.6190, 0.6775], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 1.5422e-10.\n",
      "loss: 1.0809------------\n",
      "theta: tensor([0.5313, 0.4189, 0.4780, 0.6181, 0.6790], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.00022730579075869173\n",
      "Adjusting learning rate of group 0 to 1.5268e-10.\n",
      "loss: 0.2255------------\n",
      "theta: tensor([0.5052, 0.4111, 0.5867, 0.5457, 0.6989], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.07424958795309067\n",
      "Adjusting learning rate of group 0 to 1.5115e-10.\n",
      "loss: 0.4112------------\n",
      "theta: tensor([0.5662, 0.4052, 0.4737, 0.6401, 0.6561], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.021772731095552444\n",
      "Adjusting learning rate of group 0 to 1.4964e-10.\n",
      "loss: 1.0489------------\n",
      "theta: tensor([0.5787, 0.4005, 0.4660, 0.6547, 0.6472], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 1.4814e-10.\n",
      "loss: 0.1661------------\n",
      "theta: tensor([0.5784, 0.3956, 0.5081, 0.6231, 0.6508], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.00448177196085453\n",
      "Adjusting learning rate of group 0 to 1.4666e-10.\n",
      "loss: 0.0451------------\n",
      "theta: tensor([0.5480, 0.4016, 0.5397, 0.5915, 0.6739], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.02503407560288906\n",
      "Adjusting learning rate of group 0 to 1.4520e-10.\n",
      "H_final is suspiciously low tensor([ 1.8966e+00,  1.9229e+02, -1.0038e-01,  1.1333e+00],\n",
      "       grad_fn=<SubBackward0>)\n",
      "loss: 0.9358------------\n",
      "theta: tensor([0.5332, 0.4188, 0.4768, 0.6199, 0.6763], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -1.6972866433206946e-05\n",
      "Adjusting learning rate of group 0 to 1.4374e-10.\n",
      "loss: 1.0256------------\n",
      "theta: tensor([0.5242, 0.3959, 0.5500, 0.5792, 0.7222], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.020049981772899628\n",
      "Adjusting learning rate of group 0 to 1.4231e-10.\n",
      "loss: 1.0925------------\n",
      "theta: tensor([0.4874, 0.4178, 0.6074, 0.5248, 0.7015], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 7.686022581765428e-05\n",
      "Adjusting learning rate of group 0 to 1.4088e-10.\n",
      "loss: 0.0216------------\n",
      "theta: tensor([0.5669, 0.3996, 0.4960, 0.6247, 0.6647], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.01025769580155611\n",
      "Adjusting learning rate of group 0 to 1.3948e-10.\n",
      "loss: 1.0900------------\n",
      "theta: tensor([0.4927, 0.4151, 0.5993, 0.5324, 0.7032], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: 0.00033814154448919\n",
      "Adjusting learning rate of group 0 to 1.3808e-10.\n",
      "loss: 0.0354------------\n",
      "theta: tensor([0.5854, 0.3949, 0.4871, 0.6374, 0.6495], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.006688124965876341\n",
      "Adjusting learning rate of group 0 to 1.3670e-10.\n",
      "loss: 1.0502------------\n",
      "theta: tensor([0.5775, 0.4028, 0.4639, 0.6548, 0.6438], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 0, incices: [], gradients: tensor([])\n",
      "Adjusting learning rate of group 0 to 1.3533e-10.\n",
      "loss: 0.6651------------\n",
      "theta: tensor([0.5728, 0.3961, 0.5205, 0.6127, 0.6550], grad_fn=<SigmoidBackward0>)\n",
      "Num non-zero gadients: 1, incices: 4, gradients: -0.00135450204834342\n",
      "Adjusting learning rate of group 0 to 1.3398e-10.\n",
      "tensor([0.4442, 0.4812, 0.2156, 0.3182, 0.4057], requires_grad=True)\n",
      "tensor([0.5413, 0.3883, 0.5185, 0.6057, 0.7268], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bucket_nn.set_value(PRECIP_RECORD, torch.tensor(network_precip_input_list, requires_grad=False))\n",
    "y_pred, loss = train_ncnn(bucket_nn, network_precip_tensor, network_outflow_tensor_0)\n",
    "network_outflow_list_1b = list(y_pred.detach().numpy())\n",
    "print(bucket_net.theta)\n",
    "print(bucket_nn.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354211ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb89e7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x333.333 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DO_PLOT = True\n",
    "if DO_PLOT:\n",
    "    startplt = 230\n",
    "    endplt = 280\n",
    "    bigfontsize = 12\n",
    "    smallfontsize = 10\n",
    "    # Set figure size to 7 inches wide\n",
    "    figure_width = 5  # inches\n",
    "    aspect_ratio = 3 / 2  # Adjust as needed for your plot\n",
    "    figure_height = figure_width / aspect_ratio\n",
    "\n",
    "    plt.figure(figsize=(figure_width, figure_height))\n",
    "\n",
    "    # Set color\n",
    "    light_blue_color = \"#3D68CA\"\n",
    "    line_and_text_color = \"#0D3F6E\"\n",
    "\n",
    "    # Plot each series with specified color\n",
    "    plt.plot(network_precip_input_list[startplt:endplt], c=light_blue_color, lw=3, label=\"Precipitation input\")\n",
    "    plt.plot(network_outflow_list_0[startplt:endplt], c=\"k\", lw=3, label=\"Target hydrograph \")\n",
    "    plt.plot(network_outflow_list_1a[startplt:endplt], \"--\", lw=2, c=line_and_text_color, label=\"Pre-trained network\")\n",
    "    plt.plot(network_outflow_list_1b[startplt:endplt], lw=3, c=\"r\", label=\"Trained bucket network\")\n",
    "\n",
    "    network_precip_tensor = torch.tensor(network_precip_input_list)\n",
    "    max_value = torch.max(network_precip_tensor[startplt:endplt]).item()\n",
    "#    plt.ylim([0, max_value * 1.2])\n",
    "\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.ylabel(\"Unit hydrograph\", fontsize=bigfontsize, color=line_and_text_color)\n",
    "    plt.xlabel(\"Time\", fontsize=bigfontsize, color=line_and_text_color)\n",
    "\n",
    "    # Modify legend to have a transparent background\n",
    "    plt.legend(fontsize=smallfontsize, edgecolor=line_and_text_color, framealpha=0.5)  # Adjust framealpha as needed\n",
    "\n",
    "#    plt.title(\"Flow out from network and precip\", fontsize=bigfontsize, color=line_and_text_color)\n",
    "\n",
    "    # Save the figure with transparent background and at 300 DPI\n",
    "    plt.show()\n",
    "    #plt.savefig(\"ncn_plot.png\", transparent=True, dpi=300)\n",
    "\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd9eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d717beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'H': tensor([13.8633], grad_fn=<ViewBackward0>),\n",
       "  'S': tensor([[[0.4963, 0.7682],\n",
       "           [0.0885, 0.1320]]]),\n",
       "  's_q': tensor([[4.9068e+00, 2.7026e-03]], grad_fn=<SliceBackward0>)},\n",
       " 1: {'H': tensor([10.1006,  1.9601], grad_fn=<ViewBackward0>),\n",
       "  'S': tensor([[[0.3074, 0.6341]],\n",
       "  \n",
       "          [[0.4901, 0.8964]]]),\n",
       "  's_q': tensor([[1.2266],\n",
       "          [0.3574]], grad_fn=<SliceBackward0>)},\n",
       " 2: {'H': tensor([2.3368], grad_fn=<ViewBackward0>),\n",
       "  'S': tensor([[[0.4556, 0.6323]]]),\n",
       "  's_q': tensor([[0.3389]], grad_fn=<SliceBackward0>)}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_net.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a91cb5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'H': tensor([94.9592], grad_fn=<ViewBackward0>),\n",
       "  'S': tensor([[[0.6650, 0.0336],\n",
       "           [0.3315, 0.1784]]]),\n",
       "  's_q': tensor([[2.1750e-09, 1.2629e+00]], grad_fn=<SliceBackward0>)},\n",
       " 1: {'H': tensor([92.9934,  2.8395], grad_fn=<ViewBackward0>),\n",
       "  'S': tensor([[[0.8453, 0.2159]],\n",
       "  \n",
       "          [[0.4880, 0.9444]]]),\n",
       "  's_q': tensor([[0.4657],\n",
       "          [0.5162]], grad_fn=<SliceBackward0>)},\n",
       " 2: {'H': tensor([88.6425], grad_fn=<ViewBackward0>),\n",
       "  'S': tensor([[[0.8790, 0.1403]]]),\n",
       "  's_q': tensor([[7.7189e-08]], grad_fn=<SliceBackward0>)}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_nn.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "568b565c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4442, 0.4812, 0.2156, 0.3182, 0.4057], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_net.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3befa50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5413, 0.3883, 0.5185, 0.6057, 0.7268], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_nn.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d204aa2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5416, 0.4018, 0.5636, 0.5740, 0.6751])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origional_bucket_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "888d8141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0247, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(bucket_nn.theta - origional_bucket_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "553672bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0002, -0.0134, -0.0451,  0.0318,  0.0516], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_nn.theta - origional_bucket_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4455c9a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'thislinewillstopthenotebookfromrunningthecellsbelow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mthislinewillstopthenotebookfromrunningthecellsbelow\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'thislinewillstopthenotebookfromrunningthecellsbelow' is not defined"
     ]
    }
   ],
   "source": [
    "print(thislinewillstopthenotebookfromrunningthecellsbelow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a7c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N_TIMESTEPS):\n",
    "\n",
    "    ###########################################################################\n",
    "    ###########################################################################\n",
    "    bucket_nn.set_value(PRECIP_SVN, torch.tensor(network_precip_input_list[i], requires_grad=True))\n",
    "    if bucket_nn.do_predict_theta_with_lstm:\n",
    "        sequence_tensors = []\n",
    "        tensor_device = bucket_nn.network[0]['H'].device\n",
    "        tensor_dtype = torch.float32\n",
    "        if i >= bucket_nn.input_u_sequence_length:\n",
    "            sequence_tensors = [torch.tensor([item], device=tensor_device, dtype=tensor_dtype) \n",
    "                                for item in network_precip_input_list[i-bucket_nn.input_u_sequence_length:i]]\n",
    "        else:\n",
    "            desired_tensor_shape = (1,)\n",
    "            padding_size = bucket_nn.input_u_sequence_length - i\n",
    "            padding_tensors = [torch.zeros(desired_tensor_shape, device=tensor_device, dtype=tensor_dtype) \n",
    "                            for _ in range(padding_size)]\n",
    "            sequence_tensors = padding_tensors + [torch.tensor([item], device=tensor_device, dtype=tensor_dtype) \n",
    "                                                for item in network_precip_input_list[:i]]\n",
    "        sequence = torch.stack(sequence_tensors).view(1, -1)\n",
    "        bucket_nn.set_value(PRECIP_SVN_SEQ, sequence)\n",
    "\n",
    "\n",
    "    bucket_nn.update()\n",
    "    network_outflow_list_1b.append(bucket_nn.network_outflow.item())\n",
    "    bucket_nn.summarize_network()\n",
    "\n",
    "    if i in [180, 200]:\n",
    "        print(bucket_nn.theta)\n",
    "        print(bucket_nn.get_the_H_tensor())\n",
    "    ###########################################################################\n",
    "    ###########################################################################\n",
    "\n",
    "    if DO_PLOT:\n",
    "        if i % int(N_TIMESTEPS/10) == 0:\n",
    "            plt.plot([tensor.item() for tensor in bucket_nn.mean_H_per_layer])\n",
    "\n",
    "###########################################################################\n",
    "network_outflow_tensor_1 = torch.tensor(network_outflow_list_1b, requires_grad=True)\n",
    "bucket_nn.report_out_mass_balance()\n",
    "\n",
    "origional_bucket_theta = copy.deepcopy(bucket_nn.theta.detach())\n",
    "\n",
    "if DO_PLOT:\n",
    "    plt.title(\"Mean head in each bucket per layer\")\n",
    "    plt.ylabel(\"Average head per layer\")\n",
    "    plt.xlabel(\"Network Layers\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "if DO_PLOT:\n",
    "    startplt = 0\n",
    "    endplt = 250\n",
    "\n",
    "    plt.plot(network_precip_input_list, c=\"grey\", lw=0.5, label=\"Precipitation input\")\n",
    "    plt.plot(network_outflow_list_1b, label=\"Flow out of network\")\n",
    "    plt.xlim([startplt, endplt])\n",
    "    plt.ylim([0, torch.max(torch.tensor(network_precip_input_list)[startplt:endplt]).item()])\n",
    "    plt.legend()\n",
    "    plt.title(\"Warmup period\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    startplt = int(len(network_outflow_list_1b)-(len(network_outflow_list_1b)/2))\n",
    "    endplt = int(len(network_outflow_list_1b))\n",
    "    plt.plot(network_precip_input_list, c=\"grey\", lw=0.5, label=\"Precipitation input\")\n",
    "    plt.plot(network_outflow_list_1b, label=\"Flow out of network\")\n",
    "    plt.xlim([startplt, endplt])\n",
    "    plt.legend()\n",
    "    plt.title(\"Flow out from network and precip\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a76eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(thiscellwillstopthenotebookbeforethepureneuralnetworksaretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e37cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.autograd import Variable \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using CUDA device: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e9bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays to PyTorch tensors\n",
    "x_input = torch.FloatTensor(network_precip_tensor.detach().numpy())\n",
    "y_target = torch.FloatTensor(network_outflow_tensor_0.detach().numpy())\n",
    "\n",
    "# Define the sequence length (you can adjust this)\n",
    "seq_length = 12\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(input_data, target_data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(input_data)-seq_length):\n",
    "        x_seq = input_data[i:i+seq_length]\n",
    "        y_seq = target_data[i+seq_length]\n",
    "        xs.append(x_seq)\n",
    "        ys.append(y_seq)\n",
    "    return torch.stack(xs), torch.stack(ys)\n",
    "\n",
    "X, y = create_sequences(x_input, y_target, seq_length)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=256, output_size=1, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers=num_layers)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq.view(len(input_seq), 1, -1))\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]\n",
    "\n",
    "model = LSTMModel()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    for seq, labels in zip(X, y):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq)\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch: {epoch} loss: {single_loss.item()}')\n",
    "\n",
    "print(f'Final loss: {single_loss.item()}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for seq in X:\n",
    "        predictions.append(model(seq).item())\n",
    "\n",
    "# Convert predictions to a numpy array for easy plotting\n",
    "predictions_np = np.array(predictions)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"LSTM Predictions vs Actual Data\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.plot(x_input.numpy()[seq_length:], label='Precipitation', alpha=0.7)\n",
    "plt.plot(y_target.numpy()[seq_length:], label='Actual Outflow', alpha=0.7)\n",
    "plt.plot(predictions_np, label='Predicted Outflow', alpha=0.7, linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays to PyTorch tensors\n",
    "x_input = torch.FloatTensor(network_precip_tensor.detach().numpy())\n",
    "y_target = torch.FloatTensor(network_outflow_tensor_0.detach().numpy())\n",
    "\n",
    "# Define the sequence length (you can adjust this)\n",
    "seq_length = 35\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(input_data, target_data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(input_data) - seq_length):\n",
    "        x_seq = input_data[i:i + seq_length]\n",
    "        y_seq = target_data[i + seq_length]\n",
    "        xs.append(x_seq)\n",
    "        ys.append(y_seq)\n",
    "    return torch.stack(xs), torch.stack(ys)\n",
    "\n",
    "X, y = create_sequences(x_input, y_target, seq_length)\n",
    "\n",
    "# Feedforward Neural Network Model\n",
    "class FFNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size=3, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        # Define your feedforward layers here\n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        self.activations = []\n",
    "        x = self.fc1(input_seq)\n",
    "        self.activations.append(x.detach().numpy())  # Record activation\n",
    "        x = self.relu1(x)\n",
    "        self.activations.append(x.detach().numpy())  # Record activation\n",
    "        x = self.fc2(x)\n",
    "        self.activations.append(x.detach().numpy())  # Record activation\n",
    "        x = self.relu2(x)\n",
    "        self.activations.append(x.detach().numpy())  # Record activation\n",
    "        x = self.fc3(x)\n",
    "        self.activations.append(x.detach().numpy())  # Record activation\n",
    "        predictions = self.fc4(x)\n",
    "        return predictions[-1]\n",
    "\n",
    "model = FFNNModel(input_size=seq_length)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.006)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.99, verbose=True)\n",
    "\n",
    "epochs = 150\n",
    "for epoch in range(epochs):\n",
    "    for seq, labels in zip(X, y):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq)\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch: {epoch} loss: {single_loss.item():.4f}')\n",
    "\n",
    "print(f'Final loss: {single_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d416ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input.shape, y_target.shape, predictions_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f70ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, seq in enumerate(X):\n",
    "        predictions.append(model(seq).item())\n",
    "predictions_np = np.array(predictions)\n",
    "\n",
    "DO_PLOT = True\n",
    "if DO_PLOT:\n",
    "    startplt = 230\n",
    "    endplt = 280\n",
    "    bigfontsize = 22\n",
    "    smallfontsize = 16\n",
    "    # Set figure size to 7 inches wide\n",
    "    figure_width = 7  # inches\n",
    "    aspect_ratio = 3 / 2  # Adjust as needed for your plot\n",
    "    figure_height = figure_width / aspect_ratio\n",
    "\n",
    "    plt.figure(figsize=(figure_width, figure_height))\n",
    "\n",
    "    # Set color\n",
    "    light_blue_color = \"#3D68CA\"\n",
    "    line_and_text_color = \"#0D3F6E\"\n",
    "\n",
    "    # Plot each series with specified color\n",
    "    plt.plot(network_precip_input_list[startplt:endplt], c=light_blue_color, lw=10, label=\"Precipitation input\")\n",
    "    plt.plot(network_outflow_list_0[startplt:endplt], c=\"k\", lw=10, label=\"Target hydrograph \")\n",
    "    # plt.plot(network_outflow_list_1a[startplt:endplt], \"--\", lw=5, c=line_and_text_color, label=\"Pre-trained network\")\n",
    "    plt.plot(predictions_np[startplt-seq_length:endplt-seq_length], \"--\", lw=10, c=line_and_text_color, label=\"Trained bucket network\")\n",
    "\n",
    "    network_precip_tensor = torch.tensor(network_precip_input_list)\n",
    "    max_value = torch.max(network_precip_tensor[startplt:endplt]).item()\n",
    "    plt.ylim([0, max_value * 1.2])\n",
    "\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.ylabel(\"Unit hydrograph\", fontsize=bigfontsize, color=line_and_text_color)\n",
    "    plt.xlabel(\"Time\", fontsize=bigfontsize, color=line_and_text_color)\n",
    "\n",
    "    # Modify legend to have a transparent background\n",
    "    #plt.legend(fontsize=smallfontsize, edgecolor=line_and_text_color, framealpha=0.5)  # Adjust framealpha as needed\n",
    "\n",
    "    #plt.title(\"Flow out from network and precip\", fontsize=bigfontsize, color=line_and_text_color)\n",
    "\n",
    "    # Save the figure with transparent background and at 300 DPI\n",
    "    #plt.show()\n",
    "    plt.savefig(\"./figs/ffn_plot.png\", transparent=True, dpi=300)\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot_the_time_series(x_input, y_target, predictions_np, seq_length):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(\"FFNN Predictions vs Actual Data\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Output\")\n",
    "    plt.plot(x_input.numpy()[seq_length:], label='Precipitation', alpha=0.7)\n",
    "    plt.plot(y_target.numpy()[seq_length:], label='Actual Outflow', alpha=0.7)\n",
    "    plt.plot(predictions_np, label='Predicted Outflow', alpha=0.7, linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, seq in enumerate(X):\n",
    "        predictions.append(model(seq).item())\n",
    "predictions_np = np.array(predictions)\n",
    "plot_the_time_series(x_input, y_target, predictions_np, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac67c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.cm import ScalarMappable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4548040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(thiscellwillstopthenotebookbeforetheanimationismade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer_connections_with_annotations(model, input_seq, title):\n",
    "    model(input_seq)  # Forward pass to record activations\n",
    "    activations = model.activations\n",
    "\n",
    "    # Concatenate activations horizontally\n",
    "    concatenated_activations = np.hstack([activation.reshape(-1, 1) for activation in activations])\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(3, 2))  # Adjust the figure size as needed\n",
    "    ax = sns.heatmap(concatenated_activations, annot=True, fmt=\".2f\", cmap=\"viridis\", annot_kws={\"size\": 8})\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Layer Neurons\")\n",
    "    plt.ylabel(\"Activation\")\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "for i in [50, 75]:\n",
    "    visualize_layer_connections_with_annotations(model, X[i], f\"Neuron Connections at x={i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57851cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5503f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891b699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_visualization_subplot(model, input_seq, layer_sizes, title, ax):\n",
    "    model(input_seq)  # Forward pass to record activations\n",
    "    activations = model.activations\n",
    "\n",
    "    # Create a color map\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "\n",
    "    # Normalize the activation values to [-5, 5] for the color map\n",
    "    norm = mcolors.Normalize(vmin=-5, vmax=5)\n",
    "\n",
    "    # Number of layers including input and output\n",
    "    num_layers = len(layer_sizes)\n",
    "\n",
    "    # List to store node positions, to be used to draw the edges\n",
    "    node_positions = {}\n",
    "\n",
    "    # Generate positions for each layer\n",
    "    for i, size in enumerate(layer_sizes):\n",
    "        # Vertical positions\n",
    "        v_positions = np.linspace(0, 1, size + 2)[1:-1]\n",
    "\n",
    "        # Horizontal position\n",
    "        h_position = i / (num_layers - 1)\n",
    "\n",
    "        # Draw nodes\n",
    "        for j, v in enumerate(v_positions):\n",
    "            activation = 0\n",
    "            if i > 0:  # Skip input layer for activations\n",
    "                activation = activations[i-1][j]\n",
    "            color = cmap(norm(activation))\n",
    "\n",
    "            circle = plt.Circle((h_position, v), 0.05, color=color, zorder=4)\n",
    "            ax.add_artist(circle)\n",
    "            node_positions[(i, j)] = (h_position, v)\n",
    "\n",
    "            # Optionally, add activation values as text inside the nodes\n",
    "            # plt.text(h_position, v, f'{activation:.2f}', ha='center', va='center', color='white', fontsize=8)\n",
    "\n",
    "    # Draw edges\n",
    "    for i in range(num_layers - 1):\n",
    "        for j in range(layer_sizes[i]):\n",
    "            for k in range(layer_sizes[i + 1]):\n",
    "                start_pos = node_positions[(i, j)]\n",
    "                end_pos = node_positions[(i + 1, k)]\n",
    "                line = plt.Line2D([start_pos[0], end_pos[0]], [start_pos[1], end_pos[1]], c='black', alpha=0.3)\n",
    "                ax.add_line(line)\n",
    "\n",
    "    mappable = ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "    # Add a colorbar\n",
    "    plt.colorbar(mappable, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Set title and turn off axis\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9741cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_time_series_subplot(x_input, y_target, predictions_np, seq_length, t, ax):\n",
    "    ax.set_title(\"FFNN Predictions vs Actual Data\")\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "    ax.set_ylabel(\"Output\")\n",
    "    ax.plot(x_input.numpy()[seq_length:], label='Precipitation', alpha=0.7)\n",
    "    ax.plot(y_target.numpy()[seq_length:], label='Actual Outflow', alpha=0.7)\n",
    "    ax.plot(predictions_np, label='Predicted Outflow', alpha=0.7, linestyle='--')\n",
    "    ax.axvline(x=t, color='red', linestyle='--')  # Vertical line\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bda8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of each layer (including input and output layers)\n",
    "layer_sizes = [1, 3, 3, 3, 1]  # Example: input layer with 35 nodes, three hidden layers with 3 nodes each, and an output layer with 1 node\n",
    "\n",
    "# Usage\n",
    "for t in range(1,200):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "    # Network visualization on the first subplot\n",
    "    network_visualization_subplot(model, X[t], layer_sizes, f\"Neural Network Visualization at t={t}\", ax1)\n",
    "\n",
    "    # Time series plot on the second subplot\n",
    "    plot_the_time_series_subplot(x_input, y_target, predictions_np, seq_length, t, ax2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./figs/network_plot/combined_frame_{t}.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64095e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [f\"./figs/network_plot/combined_frame_{t}.png\" for t in range(1, 200)]\n",
    "images = [imageio.imread(filename) for filename in filenames]\n",
    "imageio.mimsave('network_animation.gif', images, fps=2)  # Adjust fps as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7e6a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382c53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

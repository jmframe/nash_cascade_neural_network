{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45cb35ce-803e-43be-b263-4a1a0f3bb6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nash Cascade Neural Network\n",
    "# A hydrologically intuitive deep learning network\n",
    "\n",
    "# Set up a solution to a network of buckets where the number of buckets in each layer\n",
    "# flows out to the buckets in the next layer\n",
    "# The parameter on each bucket is the size and height of each spigot.\n",
    "\n",
    "# Need a function that solves this individually at a single buckets\n",
    "# Then a function that loops through and moves the water to the downstream buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62bab940-cde5-4ff0-8897-75fdf2ebec52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from ncn_lstm import NashCascadeNetwork as ncn\n",
    "from ncn_lstm import train_model as train_ncnn\n",
    "import matplotlib.font_manager as font_manager\n",
    "# Precipitation standard variable name used in the ncnn model interface\n",
    "PRECIP_SVN = \"atmosphere_water__liquid_equivalent_precipitation_rate\"\n",
    "PRECIP_SVN_SEQ = \"atmosphere_water__liquid_equivalent_precipitation_rate_seq\"\n",
    "PRECIP_RECORD = \"atmosphere_water__liquid_equivalent_precipitation_rate_record\"\n",
    "DO_PLOT = True\n",
    "N_TIMESTEPS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46bcc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_precip_input_list = []\n",
    "count = 0\n",
    "unit_precip = 6.0\n",
    "for i in range(N_TIMESTEPS):\n",
    "\n",
    "    ###########################################################################\n",
    "    if count == 0:\n",
    "        network_precip_input_list.append(1.0)\n",
    "    elif count > 45:\n",
    "        network_precip_input_list.append(unit_precip)\n",
    "    else:\n",
    "        network_precip_input_list.append(0.0)\n",
    "    if count == 50:\n",
    "        count = 0\n",
    "    count+=1\n",
    "    ###########################################################################\n",
    "network_precip_tensor = torch.tensor(network_precip_input_list, requires_grad=False)\n",
    "total_mass_precip_in = torch.sum(network_precip_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92cb4cbf-61b3-4566-a9e0-37d15e846fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Mass in network at start: 25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonat/nash_cascade_neural_network/ncn_lstm.py:367: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inflows_tensor = torch.tensor(inflows, dtype=torch.float32)  # Convert inflows to a tensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Mass in network: 25.0\n",
      "Final Mass in network: 22.9\n",
      "Total Mass out of network 177.1\n",
      "Total precipitation into network 175.0\n",
      "Mass balance for network is -0.000\n",
      "Mass balance for network is -0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.1882, 0.2556, 0.3093, 0.3400, 0.1483, 0.1847, 0.3045])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 0\n",
    "bucket_net = ncn(cfg_file=\"./config_0.json\")\n",
    "bucket_net.initialize()\n",
    "bucket_net.unit_precip = unit_precip\n",
    "bucket_net.summarize_network()\n",
    "inital_mass_in_network = torch.sum(torch.tensor([tensor.item() for tensor in bucket_net.sum_H_per_layer]))\n",
    "print(f\"Initial Mass in network at start: {inital_mass_in_network:.1f}\")\n",
    "network_outflow_list_0 = []\n",
    "for i in range(N_TIMESTEPS):\n",
    "\n",
    "    ###########################################################################\n",
    "    ###########################################################################\n",
    "    bucket_net.set_value(PRECIP_SVN, torch.tensor(network_precip_input_list[i], requires_grad=False))\n",
    "    bucket_net.update()\n",
    "    network_outflow_list_0.append(bucket_net.network_outflow.item())\n",
    "    bucket_net.summarize_network()\n",
    "    ###########################################################################\n",
    "    ###########################################################################\n",
    "\n",
    "network_outflow_tensor_0 = torch.tensor(network_outflow_list_0, requires_grad=True)\n",
    "bucket_net.report_out_mass_balance()\n",
    "bucket_net.detach_ncn_from_graph()\n",
    "bucket_net.theta.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "744fc71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Initial Mass in network at start: 25.0\n",
      "Initial Mass in network: 25.0\n",
      "Final Mass in network: 18.0\n",
      "Total Mass out of network 182.0\n",
      "Total precipitation into network 175.0\n",
      "Mass balance for network is -0.000\n",
      "Mass balance for network is -0.000\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "bucket_nn = ncn(cfg_file=\"./config_1_lstm.json\")\n",
    "bucket_nn.initialize()\n",
    "bucket_nn.unit_precip = unit_precip\n",
    "print(\"Initialized\")\n",
    "bucket_nn.initialize_theta_values()\n",
    "inital_mass_in_network = torch.sum(torch.stack(bucket_nn.sum_H_per_layer)).item()\n",
    "print(f\"Initial Mass in network at start: {inital_mass_in_network:.1f}\")\n",
    "network_outflow_list_1a = []\n",
    "for i in range(N_TIMESTEPS):\n",
    "\n",
    "    ###########################################################################\n",
    "    ###########################################################################\n",
    "    bucket_nn.set_value(PRECIP_SVN, torch.tensor(network_precip_input_list[i], requires_grad=True))\n",
    "    if bucket_nn.do_predict_theta_with_lstm:\n",
    "        sequence_tensors = []\n",
    "        tensor_device = bucket_nn.network[0]['H'].device\n",
    "        tensor_dtype = torch.float32\n",
    "        if i >= bucket_nn.input_u_sequence_length:\n",
    "            sequence_tensors = [torch.tensor([item], device=tensor_device, dtype=tensor_dtype) \n",
    "                                for item in network_precip_input_list[i-bucket_nn.input_u_sequence_length:i]]\n",
    "        else:\n",
    "            desired_tensor_shape = (1,)\n",
    "            padding_size = bucket_nn.input_u_sequence_length - i\n",
    "            padding_tensors = [torch.zeros(desired_tensor_shape, device=tensor_device, dtype=tensor_dtype) \n",
    "                            for _ in range(padding_size)]\n",
    "            sequence_tensors = padding_tensors + [torch.tensor([item], device=tensor_device, dtype=tensor_dtype) \n",
    "                                                for item in network_precip_input_list[:i]]\n",
    "        sequence = torch.stack(sequence_tensors).view(1, -1)\n",
    "        bucket_nn.set_value(PRECIP_SVN_SEQ, sequence)\n",
    "\n",
    "\n",
    "    bucket_nn.update()\n",
    "    network_outflow_list_1a.append(bucket_nn.network_outflow.item())\n",
    "    bucket_nn.summarize_network()\n",
    "    ###########################################################################\n",
    "    ###########################################################################\n",
    "\n",
    "###########################################################################\n",
    "network_outflow_tensor_1 = torch.tensor(network_outflow_list_1a, requires_grad=True)\n",
    "bucket_nn.report_out_mass_balance()\n",
    "\n",
    "origional_bucket_theta = copy.deepcopy(bucket_nn.theta.detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "217de549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL MODEL theta: tensor([0.4727, 0.5021, 0.4821, 0.5319, 0.5291, 0.5217, 0.5212],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "loss: 0.7830------------\n",
      "theta: tensor([0.4727, 0.5021, 0.4821, 0.5319, 0.5291, 0.5217, 0.5212],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "loss: 0.2353------------\n",
      "theta: tensor([0.4379, 0.6466, 0.4619, 0.6137, 0.4957, 0.5101, 0.0096],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 9.0000e-02.\n",
      "loss: 0.0898------------\n",
      "theta: tensor([0.4295, 0.5798, 0.4423, 0.5599, 0.4700, 0.5047, 0.1118],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 9.0000e-02.\n",
      "loss: 0.7518------------\n",
      "theta: tensor([0.4502, 0.5243, 0.4487, 0.5200, 0.4642, 0.5282, 0.5198],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 8.1000e-02.\n",
      "loss: 1.3262------------\n",
      "theta: tensor([0.4547, 0.5087, 0.4302, 0.5220, 0.4598, 0.5292, 0.3729],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 8.1000e-02.\n",
      "loss: 0.6703------------\n",
      "theta: tensor([0.4484, 0.5216, 0.4273, 0.5311, 0.4680, 0.5314, 0.1887],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 7.2900e-02.\n",
      "loss: 0.1006------------\n",
      "theta: tensor([0.4389, 0.5248, 0.4291, 0.5316, 0.4758, 0.5479, 0.0827],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 7.2900e-02.\n",
      "loss: 0.2576------------\n",
      "theta: tensor([0.4323, 0.5156, 0.4300, 0.5287, 0.4684, 0.5606, 0.0502],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 6.5610e-02.\n",
      "loss: 0.1400------------\n",
      "theta: tensor([0.4302, 0.5263, 0.4321, 0.5344, 0.4754, 0.5512, 0.0281],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 6.5610e-02.\n",
      "loss: 0.4046------------\n",
      "theta: tensor([0.4265, 0.4996, 0.4301, 0.5266, 0.4538, 0.5666, 0.0286],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.9049e-02.\n",
      "loss: 0.0530------------\n",
      "theta: tensor([0.4266, 0.5403, 0.4396, 0.5493, 0.4846, 0.5302, 0.0130],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.9049e-02.\n",
      "loss: 0.1430------------\n",
      "theta: tensor([0.4222, 0.5105, 0.4366, 0.5377, 0.4675, 0.5518, 0.0180],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.3144e-02.\n",
      "loss: 0.1067------------\n",
      "theta: tensor([0.4247, 0.5013, 0.4352, 0.5428, 0.4670, 0.5475, 0.0191],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.3144e-02.\n",
      "loss: 0.2252------------\n",
      "theta: tensor([0.4443, 0.4657, 0.4112, 0.5370, 0.4481, 0.5364, 0.0288],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.7830e-02.\n",
      "loss: 0.1758------------\n",
      "theta: tensor([0.4480, 0.4609, 0.4073, 0.5360, 0.4496, 0.5228, 0.0330],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.7830e-02.\n",
      "loss: 0.0509------------\n",
      "theta: tensor([0.4211, 0.5099, 0.4491, 0.5611, 0.4788, 0.5227, 0.0175],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.3047e-02.\n",
      "loss: 0.0683------------\n",
      "theta: tensor([0.4210, 0.5056, 0.4496, 0.5614, 0.4779, 0.5201, 0.0201],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.3047e-02.\n",
      "loss: 0.0364------------\n",
      "theta: tensor([0.4261, 0.4974, 0.4420, 0.5588, 0.4755, 0.5155, 0.0245],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.8742e-02.\n",
      "loss: 0.0332------------\n",
      "theta: tensor([0.4280, 0.4961, 0.4406, 0.5594, 0.4761, 0.5129, 0.0261],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.8742e-02.\n",
      "loss: 0.1460------------\n",
      "theta: tensor([0.4263, 0.4999, 0.4541, 0.5734, 0.4860, 0.5232, 0.0231],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.4868e-02.\n",
      "loss: 0.0651------------\n",
      "theta: tensor([0.4249, 0.5022, 0.4514, 0.5665, 0.4837, 0.5122, 0.0224],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.4868e-02.\n",
      "loss: 0.0240------------\n",
      "theta: tensor([0.4270, 0.5097, 0.4478, 0.5681, 0.4802, 0.5077, 0.0219],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.1381e-02.\n",
      "loss: 0.0550------------\n",
      "theta: tensor([0.4461, 0.4726, 0.4160, 0.5424, 0.4676, 0.4968, 0.0504],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.1381e-02.\n",
      "loss: 0.0191------------\n",
      "theta: tensor([0.4304, 0.5149, 0.4466, 0.5712, 0.4818, 0.5024, 0.0211],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.8243e-02.\n",
      "loss: 0.0253------------\n",
      "theta: tensor([0.4358, 0.4940, 0.4330, 0.5563, 0.4748, 0.5013, 0.0363],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.8243e-02.\n",
      "loss: 0.0167------------\n",
      "theta: tensor([0.4320, 0.5134, 0.4445, 0.5702, 0.4814, 0.5010, 0.0240],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.5419e-02.\n",
      "loss: 0.0148------------\n",
      "theta: tensor([0.4308, 0.5113, 0.4460, 0.5675, 0.4801, 0.4989, 0.0252],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.5419e-02.\n",
      "loss: 0.0787------------\n",
      "theta: tensor([0.4618, 0.4556, 0.3986, 0.5282, 0.4635, 0.4859, 0.0831],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.2877e-02.\n",
      "loss: 0.0823------------\n",
      "theta: tensor([0.4622, 0.4563, 0.3989, 0.5286, 0.4645, 0.4861, 0.0853],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.2877e-02.\n",
      "loss: 0.0174------------\n",
      "theta: tensor([0.4325, 0.5270, 0.4499, 0.5756, 0.4831, 0.4947, 0.0199],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.0589e-02.\n",
      "loss: 0.0339------------\n",
      "theta: tensor([0.4563, 0.4650, 0.4050, 0.5342, 0.4665, 0.4882, 0.0784],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.0589e-02.\n",
      "loss: 0.0439------------\n",
      "theta: tensor([0.4503, 0.4721, 0.4120, 0.5387, 0.4683, 0.4911, 0.0714],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.8530e-02.\n",
      "loss: 0.2421------------\n",
      "theta: tensor([0.4890, 0.4644, 0.4164, 0.5207, 0.4883, 0.5154, 0.1447],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.8530e-02.\n",
      "loss: 0.0125------------\n",
      "theta: tensor([0.4378, 0.5016, 0.4323, 0.5601, 0.4770, 0.4960, 0.0422],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.6677e-02.\n",
      "loss: 0.0366------------\n",
      "theta: tensor([0.4340, 0.5173, 0.4438, 0.5733, 0.4826, 0.4979, 0.0299],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.6677e-02.\n",
      "loss: 0.0456------------\n",
      "theta: tensor([0.4407, 0.4878, 0.4310, 0.5567, 0.4776, 0.5038, 0.0585],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.5009e-02.\n",
      "loss: 0.0652------------\n",
      "theta: tensor([0.4350, 0.5117, 0.4389, 0.5686, 0.4800, 0.4965, 0.0354],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.5009e-02.\n",
      "loss: 0.0593------------\n",
      "theta: tensor([0.4337, 0.5142, 0.4406, 0.5700, 0.4798, 0.4963, 0.0338],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.3509e-02.\n",
      "loss: 0.0382------------\n",
      "theta: tensor([0.4403, 0.4923, 0.4269, 0.5546, 0.4744, 0.4954, 0.0536],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.3509e-02.\n",
      "loss: 0.0110------------\n",
      "theta: tensor([0.4401, 0.4939, 0.4264, 0.5557, 0.4744, 0.4943, 0.0514],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.2158e-02.\n",
      "loss: 0.0140------------\n",
      "theta: tensor([0.4371, 0.5009, 0.4315, 0.5616, 0.4763, 0.4955, 0.0442],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.2158e-02.\n",
      "loss: 0.0115------------\n",
      "theta: tensor([0.4358, 0.5042, 0.4345, 0.5656, 0.4777, 0.4969, 0.0408],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.0942e-02.\n",
      "loss: 0.0251------------\n",
      "theta: tensor([0.4545, 0.4692, 0.4068, 0.5371, 0.4675, 0.4875, 0.0809],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.0942e-02.\n",
      "loss: 0.2474------------\n",
      "theta: tensor([0.4686, 0.4495, 0.3932, 0.5234, 0.4631, 0.4829, 0.1119],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 9.8477e-03.\n",
      "loss: 0.0131------------\n",
      "theta: tensor([0.4337, 0.5031, 0.4338, 0.5625, 0.4753, 0.4960, 0.0428],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 9.8477e-03.\n",
      "loss: 0.0256------------\n",
      "theta: tensor([0.4465, 0.4790, 0.4153, 0.5434, 0.4693, 0.4903, 0.0688],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 8.8629e-03.\n",
      "loss: 0.0570------------\n",
      "theta: tensor([0.4588, 0.4617, 0.4013, 0.5310, 0.4649, 0.4853, 0.0941],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 8.8629e-03.\n",
      "loss: 0.0578------------\n",
      "theta: tensor([0.4647, 0.4546, 0.3959, 0.5264, 0.4637, 0.4835, 0.1063],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 7.9766e-03.\n",
      "loss: 0.0446------------\n",
      "theta: tensor([0.4596, 0.4609, 0.4005, 0.5307, 0.4648, 0.4851, 0.0968],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 7.9766e-03.\n",
      "loss: 0.0164------------\n",
      "theta: tensor([0.4324, 0.5232, 0.4435, 0.5774, 0.4794, 0.4932, 0.0279],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 7.1790e-03.\n",
      "loss: 0.0120------------\n",
      "theta: tensor([0.4375, 0.4982, 0.4284, 0.5594, 0.4744, 0.4936, 0.0489],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 7.1790e-03.\n",
      "loss: 0.0147------------\n",
      "theta: tensor([0.4347, 0.5021, 0.4321, 0.5612, 0.4745, 0.4934, 0.0453],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 6.4611e-03.\n",
      "loss: 0.0150------------\n",
      "theta: tensor([0.4318, 0.5184, 0.4428, 0.5774, 0.4800, 0.4960, 0.0313],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 6.4611e-03.\n",
      "loss: 0.0244------------\n",
      "theta: tensor([0.4318, 0.5151, 0.4399, 0.5724, 0.4777, 0.4940, 0.0342],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.8150e-03.\n",
      "loss: 0.0195------------\n",
      "theta: tensor([0.4544, 0.4700, 0.4057, 0.5369, 0.4667, 0.4859, 0.0851],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.8150e-03.\n",
      "loss: 0.0592------------\n",
      "theta: tensor([0.4628, 0.4572, 0.3978, 0.5284, 0.4645, 0.4841, 0.1065],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.2335e-03.\n",
      "loss: 0.0484------------\n",
      "theta: tensor([0.4600, 0.4610, 0.4000, 0.5305, 0.4647, 0.4843, 0.1002],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.2335e-03.\n",
      "loss: 0.1121------------\n",
      "theta: tensor([0.4679, 0.4503, 0.3935, 0.5237, 0.4634, 0.4828, 0.1196],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.7101e-03.\n",
      "loss: 0.0567------------\n",
      "theta: tensor([0.4628, 0.4571, 0.3975, 0.5281, 0.4642, 0.4837, 0.1082],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.7101e-03.\n",
      "loss: 0.0759------------\n",
      "theta: tensor([0.4631, 0.4556, 0.3971, 0.5271, 0.4637, 0.4840, 0.1121],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.2391e-03.\n",
      "loss: 0.0571------------\n",
      "theta: tensor([0.4499, 0.4738, 0.4115, 0.5368, 0.4672, 0.4872, 0.0832],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.2391e-03.\n",
      "loss: 0.0217------------\n",
      "theta: tensor([0.4335, 0.5057, 0.4352, 0.5678, 0.4766, 0.4955, 0.0439],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.8152e-03.\n",
      "loss: 0.0104------------\n",
      "theta: tensor([0.4410, 0.4917, 0.4215, 0.5530, 0.4715, 0.4899, 0.0599],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.8152e-03.\n",
      "loss: 0.0650------------\n",
      "theta: tensor([0.4627, 0.4573, 0.3975, 0.5279, 0.4640, 0.4836, 0.1122],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.4337e-03.\n",
      "loss: 0.0079------------\n",
      "theta: tensor([0.4328, 0.5126, 0.4379, 0.5731, 0.4779, 0.4951, 0.0388],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.4337e-03.\n",
      "loss: 0.0180------------\n",
      "theta: tensor([0.4312, 0.5132, 0.4401, 0.5707, 0.4755, 0.4939, 0.0390],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.0903e-03.\n",
      "loss: 0.0165------------\n",
      "theta: tensor([0.4335, 0.5080, 0.4340, 0.5667, 0.4752, 0.4931, 0.0442],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.0903e-03.\n",
      "loss: 0.0468------------\n",
      "theta: tensor([0.4375, 0.4927, 0.4255, 0.5536, 0.4715, 0.4928, 0.0616],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.7813e-03.\n",
      "loss: 0.0112------------\n",
      "theta: tensor([0.4442, 0.4860, 0.4173, 0.5490, 0.4704, 0.4890, 0.0692],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.7813e-03.\n",
      "loss: 0.0140------------\n",
      "theta: tensor([0.4351, 0.5020, 0.4317, 0.5638, 0.4753, 0.4943, 0.0502],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.5032e-03.\n",
      "loss: 0.0142------------\n",
      "theta: tensor([0.4387, 0.4953, 0.4264, 0.5585, 0.4740, 0.4933, 0.0578],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.5032e-03.\n",
      "loss: 0.0120------------\n",
      "theta: tensor([0.4430, 0.4885, 0.4190, 0.5509, 0.4711, 0.4893, 0.0666],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.2528e-03.\n",
      "loss: 0.2084------------\n",
      "theta: tensor([0.4685, 0.4497, 0.3932, 0.5234, 0.4634, 0.4828, 0.1310],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.2528e-03.\n",
      "loss: 0.0457------------\n",
      "theta: tensor([0.4331, 0.5133, 0.4359, 0.5710, 0.4767, 0.4930, 0.0407],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.0276e-03.\n",
      "loss: 0.0142------------\n",
      "theta: tensor([0.4396, 0.4940, 0.4234, 0.5551, 0.4722, 0.4907, 0.0607],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.0276e-03.\n",
      "loss: 0.0211------------\n",
      "theta: tensor([0.4318, 0.5274, 0.4462, 0.5820, 0.4798, 0.4927, 0.0289],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.8248e-03.\n",
      "loss: 0.0412------------\n",
      "theta: tensor([0.4302, 0.5261, 0.4464, 0.5803, 0.4784, 0.4933, 0.0303],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.8248e-03.\n",
      "loss: 0.0378------------\n",
      "theta: tensor([0.4304, 0.5224, 0.4431, 0.5784, 0.4784, 0.4942, 0.0334],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.6423e-03.\n",
      "loss: 0.0487------------\n",
      "theta: tensor([0.4312, 0.5193, 0.4406, 0.5761, 0.4779, 0.4937, 0.0358],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.6423e-03.\n",
      "loss: 0.0256------------\n",
      "theta: tensor([0.4281, 0.5170, 0.4471, 0.5754, 0.4767, 0.4940, 0.0353],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.4781e-03.\n",
      "loss: 0.0469------------\n",
      "theta: tensor([0.4269, 0.5080, 0.4452, 0.5713, 0.4777, 0.4978, 0.0425],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.4781e-03.\n",
      "loss: 0.0342------------\n",
      "theta: tensor([0.4304, 0.5225, 0.4439, 0.5792, 0.4785, 0.4936, 0.0326],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.3303e-03.\n",
      "loss: 0.0124------------\n",
      "theta: tensor([0.4378, 0.4966, 0.4256, 0.5575, 0.4726, 0.4914, 0.0579],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.3303e-03.\n",
      "loss: 0.0148------------\n",
      "theta: tensor([0.4361, 0.4971, 0.4276, 0.5577, 0.4726, 0.4927, 0.0578],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.1973e-03.\n",
      "loss: 0.0154------------\n",
      "theta: tensor([0.4324, 0.5103, 0.4352, 0.5681, 0.4749, 0.4925, 0.0436],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.1973e-03.\n",
      "loss: 0.0330------------\n",
      "theta: tensor([0.4327, 0.5116, 0.4349, 0.5701, 0.4758, 0.4930, 0.0425],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.0775e-03.\n",
      "loss: 0.0579------------\n",
      "theta: tensor([0.4559, 0.4667, 0.4049, 0.5356, 0.4669, 0.4862, 0.0994],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.0775e-03.\n",
      "loss: 0.0113------------\n",
      "theta: tensor([0.4418, 0.4885, 0.4197, 0.5511, 0.4708, 0.4898, 0.0674],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 9.6977e-04.\n",
      "loss: 0.0516------------\n",
      "theta: tensor([0.4309, 0.5208, 0.4409, 0.5775, 0.4776, 0.4932, 0.0344],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 9.6977e-04.\n",
      "loss: 0.0572------------\n",
      "theta: tensor([0.4629, 0.4567, 0.3972, 0.5278, 0.4639, 0.4836, 0.1181],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 8.7280e-04.\n",
      "loss: 0.0202------------\n",
      "theta: tensor([0.4311, 0.5142, 0.4382, 0.5733, 0.4767, 0.4944, 0.0398],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 8.7280e-04.\n",
      "loss: 0.0123------------\n",
      "theta: tensor([0.4314, 0.5245, 0.4446, 0.5833, 0.4802, 0.4947, 0.0307],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 7.8552e-04.\n",
      "loss: 0.0101------------\n",
      "theta: tensor([0.4300, 0.5215, 0.4429, 0.5777, 0.4769, 0.4930, 0.0336],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 7.8552e-04.\n",
      "loss: 0.0207------------\n",
      "theta: tensor([0.4333, 0.5041, 0.4322, 0.5631, 0.4736, 0.4922, 0.0493],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 7.0697e-04.\n",
      "loss: 0.0111------------\n",
      "theta: tensor([0.4357, 0.4980, 0.4280, 0.5594, 0.4730, 0.4930, 0.0562],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 7.0697e-04.\n",
      "loss: 0.0526------------\n",
      "theta: tensor([0.4560, 0.4647, 0.4035, 0.5332, 0.4652, 0.4857, 0.1036],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 6.3627e-04.\n",
      "loss: 0.0438------------\n",
      "theta: tensor([0.4489, 0.4771, 0.4125, 0.5438, 0.4692, 0.4888, 0.0824],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 6.3627e-04.\n",
      "loss: 0.0330------------\n",
      "theta: tensor([0.4302, 0.5235, 0.4435, 0.5806, 0.4784, 0.4939, 0.0321],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.7264e-04.\n",
      "loss: 0.0240------------\n",
      "theta: tensor([0.4294, 0.5176, 0.4405, 0.5750, 0.4762, 0.4943, 0.0374],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.7264e-04.\n",
      "loss: 0.0283------------\n",
      "theta: tensor([0.4314, 0.5234, 0.4423, 0.5808, 0.4787, 0.4935, 0.0321],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.1538e-04.\n",
      "loss: 0.0098------------\n",
      "theta: tensor([0.4348, 0.5033, 0.4296, 0.5630, 0.4735, 0.4918, 0.0504],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.1538e-04.\n",
      "loss: 0.0216------------\n",
      "theta: tensor([0.4449, 0.4823, 0.4156, 0.5459, 0.4693, 0.4882, 0.0751],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.6384e-04.\n",
      "loss: 0.0170------------\n",
      "theta: tensor([0.4350, 0.5009, 0.4310, 0.5642, 0.4749, 0.4948, 0.0521],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.6384e-04.\n",
      "loss: 0.0101------------\n",
      "theta: tensor([0.4346, 0.5031, 0.4309, 0.5648, 0.4746, 0.4934, 0.0500],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.1746e-04.\n",
      "loss: 0.0217------------\n",
      "theta: tensor([0.4312, 0.5149, 0.4382, 0.5746, 0.4769, 0.4942, 0.0389],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.1746e-04.\n",
      "loss: 0.0429------------\n",
      "theta: tensor([0.4313, 0.5192, 0.4390, 0.5764, 0.4769, 0.4927, 0.0357],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.7571e-04.\n",
      "loss: 0.1319------------\n",
      "theta: tensor([0.4799, 0.4860, 0.4268, 0.5150, 0.4874, 0.5157, 0.1808],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.7571e-04.\n",
      "loss: 0.0264------------\n",
      "theta: tensor([0.4306, 0.5187, 0.4401, 0.5756, 0.4763, 0.4924, 0.0358],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.3814e-04.\n",
      "loss: 0.1182------------\n",
      "theta: tensor([0.4681, 0.4502, 0.3934, 0.5237, 0.4634, 0.4828, 0.1300],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.3814e-04.\n",
      "loss: 0.1271------------\n",
      "theta: tensor([0.4682, 0.4501, 0.3933, 0.5236, 0.4634, 0.4828, 0.1302],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.0433e-04.\n",
      "loss: 0.3603------------\n",
      "theta: tensor([0.4686, 0.4495, 0.3932, 0.5234, 0.4631, 0.4829, 0.1308],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.0433e-04.\n",
      "loss: 0.0169------------\n",
      "theta: tensor([0.4326, 0.5100, 0.4346, 0.5704, 0.4758, 0.4937, 0.0434],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.7389e-04.\n",
      "loss: 0.0173------------\n",
      "theta: tensor([0.4327, 0.5076, 0.4350, 0.5700, 0.4762, 0.4951, 0.0452],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.7389e-04.\n",
      "loss: 0.0248------------\n",
      "theta: tensor([0.4299, 0.5198, 0.4420, 0.5780, 0.4772, 0.4937, 0.0346],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.4650e-04.\n",
      "loss: 0.0282------------\n",
      "theta: tensor([0.4360, 0.4940, 0.4258, 0.5547, 0.4711, 0.4926, 0.0617],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.4650e-04.\n",
      "loss: 0.0821------------\n",
      "theta: tensor([0.4670, 0.4517, 0.3940, 0.5245, 0.4634, 0.4828, 0.1273],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.2185e-04.\n",
      "loss: 0.0144------------\n",
      "theta: tensor([0.4317, 0.5027, 0.4323, 0.5624, 0.4730, 0.4945, 0.0519],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.2185e-04.\n",
      "loss: 0.0152------------\n",
      "theta: tensor([0.4482, 0.4789, 0.4120, 0.5440, 0.4685, 0.4873, 0.0799],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.9967e-04.\n",
      "loss: 0.0389------------\n",
      "theta: tensor([0.4334, 0.5030, 0.4346, 0.5682, 0.4766, 0.4971, 0.0493],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.9967e-04.\n",
      "loss: 0.0125------------\n",
      "theta: tensor([0.4299, 0.5209, 0.4422, 0.5782, 0.4769, 0.4936, 0.0342],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.7970e-04.\n",
      "loss: 0.0766------------\n",
      "theta: tensor([0.4656, 0.4532, 0.3951, 0.5255, 0.4636, 0.4830, 0.1243],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.7970e-04.\n",
      "loss: 0.0449------------\n",
      "theta: tensor([0.4439, 0.4799, 0.4209, 0.5487, 0.4726, 0.4954, 0.0782],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.6173e-04.\n",
      "loss: 0.0482------------\n",
      "theta: tensor([0.4307, 0.5209, 0.4399, 0.5774, 0.4768, 0.4926, 0.0347],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.6173e-04.\n",
      "loss: 0.0357------------\n",
      "theta: tensor([0.4323, 0.5127, 0.4346, 0.5706, 0.4751, 0.4918, 0.0414],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.4556e-04.\n",
      "loss: 0.0165------------\n",
      "theta: tensor([0.4329, 0.5085, 0.4323, 0.5666, 0.4738, 0.4915, 0.0455],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.4556e-04.\n",
      "loss: 0.0125------------\n",
      "theta: tensor([0.4392, 0.4939, 0.4221, 0.5549, 0.4711, 0.4895, 0.0608],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.3100e-04.\n",
      "loss: 0.0281------------\n",
      "theta: tensor([0.4312, 0.5203, 0.4402, 0.5787, 0.4779, 0.4937, 0.0347],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.3100e-04.\n",
      "loss: 0.0117------------\n",
      "theta: tensor([0.4307, 0.5229, 0.4434, 0.5823, 0.4792, 0.4948, 0.0321],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.1790e-04.\n",
      "loss: 0.0135------------\n",
      "theta: tensor([0.4293, 0.5200, 0.4425, 0.5772, 0.4765, 0.4929, 0.0344],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.1790e-04.\n",
      "loss: 0.0165------------\n",
      "theta: tensor([0.4430, 0.4857, 0.4180, 0.5484, 0.4700, 0.4886, 0.0705],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.0611e-04.\n",
      "loss: 0.1472------------\n",
      "theta: tensor([0.4680, 0.4503, 0.3934, 0.5238, 0.4636, 0.4828, 0.1304],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.0611e-04.\n",
      "loss: 0.0154------------\n",
      "theta: tensor([0.4363, 0.5000, 0.4270, 0.5606, 0.4729, 0.4910, 0.0537],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 9.5500e-05.\n",
      "loss: 0.0166------------\n",
      "theta: tensor([0.4321, 0.5016, 0.4312, 0.5610, 0.4725, 0.4940, 0.0534],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 9.5500e-05.\n",
      "loss: 0.0280------------\n",
      "theta: tensor([0.4322, 0.5122, 0.4346, 0.5702, 0.4750, 0.4916, 0.0417],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 8.5950e-05.\n",
      "loss: 0.1053------------\n",
      "theta: tensor([0.4643, 0.4546, 0.3962, 0.5257, 0.4638, 0.4832, 0.1224],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 8.5950e-05.\n",
      "loss: 0.0257------------\n",
      "theta: tensor([0.4347, 0.5017, 0.4305, 0.5643, 0.4746, 0.4941, 0.0517],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 7.7355e-05.\n",
      "loss: 0.0164------------\n",
      "theta: tensor([0.4346, 0.4997, 0.4296, 0.5583, 0.4717, 0.4914, 0.0545],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 7.7355e-05.\n",
      "loss: 0.0150------------\n",
      "theta: tensor([0.4303, 0.5213, 0.4415, 0.5774, 0.4764, 0.4921, 0.0340],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 6.9620e-05.\n",
      "loss: 0.0164------------\n",
      "theta: tensor([0.4308, 0.5166, 0.4395, 0.5762, 0.4771, 0.4941, 0.0374],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 6.9620e-05.\n",
      "loss: 0.0099------------\n",
      "theta: tensor([0.4319, 0.5119, 0.4347, 0.5691, 0.4742, 0.4915, 0.0423],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 6.2658e-05.\n",
      "loss: 0.0139------------\n",
      "theta: tensor([0.4365, 0.4996, 0.4261, 0.5592, 0.4721, 0.4901, 0.0545],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 6.2658e-05.\n",
      "loss: 0.0383------------\n",
      "theta: tensor([0.4314, 0.5156, 0.4365, 0.5731, 0.4757, 0.4923, 0.0391],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.6392e-05.\n",
      "loss: 0.0154------------\n",
      "theta: tensor([0.4313, 0.5298, 0.4454, 0.5833, 0.4784, 0.4913, 0.0279],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.6392e-05.\n",
      "loss: 0.0100------------\n",
      "theta: tensor([0.4302, 0.5233, 0.4425, 0.5786, 0.4765, 0.4920, 0.0326],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.0753e-05.\n",
      "loss: 0.0179------------\n",
      "theta: tensor([0.4306, 0.5187, 0.4399, 0.5772, 0.4771, 0.4939, 0.0361],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.0753e-05.\n",
      "loss: 0.0584------------\n",
      "theta: tensor([0.4478, 0.4768, 0.4157, 0.5461, 0.4711, 0.4924, 0.0824],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.5678e-05.\n",
      "loss: 0.0412------------\n",
      "theta: tensor([0.4594, 0.4621, 0.4005, 0.5315, 0.4649, 0.4842, 0.1077],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.5678e-05.\n",
      "loss: 0.0177------------\n",
      "theta: tensor([0.4338, 0.5062, 0.4312, 0.5658, 0.4741, 0.4917, 0.0473],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.1110e-05.\n",
      "loss: 0.1263------------\n",
      "theta: tensor([0.4682, 0.4501, 0.3933, 0.5236, 0.4634, 0.4828, 0.1306],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.1110e-05.\n",
      "loss: 0.0186------------\n",
      "theta: tensor([0.4311, 0.5217, 0.4415, 0.5804, 0.4785, 0.4941, 0.0334],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.6999e-05.\n",
      "loss: 0.0165------------\n",
      "theta: tensor([0.4401, 0.4900, 0.4217, 0.5530, 0.4712, 0.4912, 0.0657],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.6999e-05.\n",
      "loss: 0.0302------------\n",
      "theta: tensor([0.4324, 0.5125, 0.4351, 0.5717, 0.4758, 0.4926, 0.0413],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.3299e-05.\n",
      "loss: 0.0092------------\n",
      "theta: tensor([0.4316, 0.5301, 0.4456, 0.5836, 0.4785, 0.4912, 0.0276],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.3299e-05.\n",
      "loss: 0.0121------------\n",
      "theta: tensor([0.4463, 0.4818, 0.4136, 0.5456, 0.4687, 0.4872, 0.0761],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.9969e-05.\n",
      "loss: 0.0242------------\n",
      "theta: tensor([0.4349, 0.4944, 0.4268, 0.5552, 0.4712, 0.4936, 0.0618],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.9969e-05.\n",
      "loss: 0.0167------------\n",
      "theta: tensor([0.4418, 0.4871, 0.4192, 0.5495, 0.4703, 0.4889, 0.0687],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.6972e-05.\n",
      "loss: 0.0101------------\n",
      "theta: tensor([0.4331, 0.5065, 0.4317, 0.5646, 0.4732, 0.4914, 0.0475],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.6972e-05.\n",
      "loss: 0.0324------------\n",
      "theta: tensor([0.4290, 0.5218, 0.4423, 0.5783, 0.4766, 0.4938, 0.0341],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.4275e-05.\n",
      "loss: 0.0373------------\n",
      "theta: tensor([0.4601, 0.4616, 0.3997, 0.5309, 0.4646, 0.4835, 0.1085],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.4275e-05.\n",
      "loss: 0.0185------------\n",
      "theta: tensor([0.4297, 0.5062, 0.4380, 0.5670, 0.4752, 0.4933, 0.0451],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.1847e-05.\n",
      "loss: 0.0445------------\n",
      "theta: tensor([0.4313, 0.5188, 0.4384, 0.5762, 0.4767, 0.4926, 0.0363],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.1847e-05.\n",
      "loss: 0.0195------------\n",
      "theta: tensor([0.4291, 0.5298, 0.4477, 0.5842, 0.4784, 0.4931, 0.0277],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.9663e-05.\n",
      "loss: 0.0171------------\n",
      "theta: tensor([0.4293, 0.5145, 0.4388, 0.5724, 0.4751, 0.4940, 0.0401],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.9663e-05.\n",
      "loss: 0.0659------------\n",
      "theta: tensor([0.4656, 0.4537, 0.3950, 0.5258, 0.4635, 0.4829, 0.1238],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.7696e-05.\n",
      "loss: 0.0849------------\n",
      "theta: tensor([0.4673, 0.4513, 0.3938, 0.5243, 0.4633, 0.4828, 0.1283],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.7696e-05.\n",
      "loss: 0.0248------------\n",
      "theta: tensor([0.4327, 0.5105, 0.4336, 0.5694, 0.4749, 0.4922, 0.0434],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.5927e-05.\n",
      "loss: 0.0660------------\n",
      "theta: tensor([0.4649, 0.4543, 0.3955, 0.5261, 0.4635, 0.4830, 0.1226],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.5927e-05.\n",
      "loss: 0.0280------------\n",
      "theta: tensor([0.4326, 0.5030, 0.4325, 0.5611, 0.4721, 0.4923, 0.0513],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.4334e-05.\n",
      "loss: 0.0312------------\n",
      "theta: tensor([0.4304, 0.5175, 0.4388, 0.5750, 0.4761, 0.4932, 0.0375],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.4334e-05.\n",
      "loss: 0.0201------------\n",
      "theta: tensor([0.4302, 0.5223, 0.4440, 0.5824, 0.4792, 0.4955, 0.0325],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.2901e-05.\n",
      "loss: 0.0149------------\n",
      "theta: tensor([0.4355, 0.5005, 0.4275, 0.5603, 0.4725, 0.4909, 0.0535],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.2901e-05.\n",
      "loss: 0.2582------------\n",
      "theta: tensor([0.4686, 0.4496, 0.3932, 0.5234, 0.4633, 0.4828, 0.1313],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.1611e-05.\n",
      "loss: 0.0279------------\n",
      "theta: tensor([0.4290, 0.5206, 0.4420, 0.5777, 0.4766, 0.4941, 0.0350],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.1611e-05.\n",
      "loss: 0.0506------------\n",
      "theta: tensor([0.4589, 0.4618, 0.4009, 0.5310, 0.4649, 0.4843, 0.1083],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.0450e-05.\n",
      "loss: 0.0197------------\n",
      "theta: tensor([0.4331, 0.5056, 0.4309, 0.5644, 0.4733, 0.4920, 0.0486],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 1.0450e-05.\n",
      "loss: 0.0346------------\n",
      "theta: tensor([0.4537, 0.4698, 0.4056, 0.5363, 0.4659, 0.4851, 0.0947],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 9.4046e-06.\n",
      "loss: 0.0417------------\n",
      "theta: tensor([0.4303, 0.5207, 0.4405, 0.5774, 0.4767, 0.4927, 0.0346],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 9.4046e-06.\n",
      "loss: 0.0237------------\n",
      "theta: tensor([0.4394, 0.4922, 0.4234, 0.5558, 0.4723, 0.4920, 0.0625],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 8.4641e-06.\n",
      "loss: 0.1187------------\n",
      "theta: tensor([0.4682, 0.4501, 0.3933, 0.5236, 0.4633, 0.4828, 0.1305],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 8.4641e-06.\n",
      "loss: 0.0136------------\n",
      "theta: tensor([0.4469, 0.4806, 0.4127, 0.5444, 0.4683, 0.4868, 0.0779],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 7.6177e-06.\n",
      "loss: 0.0380------------\n",
      "theta: tensor([0.4546, 0.4684, 0.4052, 0.5361, 0.4662, 0.4857, 0.0967],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 7.6177e-06.\n",
      "loss: 0.0245------------\n",
      "theta: tensor([0.4546, 0.4694, 0.4050, 0.5365, 0.4662, 0.4850, 0.0950],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 6.8560e-06.\n",
      "loss: 0.0743------------\n",
      "theta: tensor([0.4327, 0.4917, 0.4348, 0.5552, 0.4790, 0.4988, 0.0620],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 6.8560e-06.\n",
      "loss: 0.0449------------\n",
      "theta: tensor([0.4314, 0.5178, 0.4376, 0.5748, 0.4761, 0.4919, 0.0371],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 6.1704e-06.\n",
      "loss: 0.0175------------\n",
      "theta: tensor([0.4329, 0.5068, 0.4319, 0.5654, 0.4736, 0.4917, 0.0471],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 6.1704e-06.\n",
      "loss: 0.0300------------\n",
      "theta: tensor([0.4526, 0.4712, 0.4070, 0.5377, 0.4666, 0.4858, 0.0922],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.5533e-06.\n",
      "loss: 0.0537------------\n",
      "theta: tensor([0.4617, 0.4590, 0.3986, 0.5297, 0.4647, 0.4839, 0.1133],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 5.5533e-06.\n",
      "loss: 0.0092------------\n",
      "theta: tensor([0.4302, 0.5155, 0.4398, 0.5757, 0.4770, 0.4951, 0.0385],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.9980e-06.\n",
      "loss: 0.0146------------\n",
      "theta: tensor([0.4397, 0.4937, 0.4214, 0.5548, 0.4710, 0.4891, 0.0611],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.9980e-06.\n",
      "loss: 0.0227------------\n",
      "theta: tensor([0.4333, 0.5078, 0.4316, 0.5662, 0.4738, 0.4912, 0.0461],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.4982e-06.\n",
      "loss: 0.0294------------\n",
      "theta: tensor([0.4461, 0.4784, 0.4137, 0.5429, 0.4682, 0.4885, 0.0816],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.4982e-06.\n",
      "loss: 0.0336------------\n",
      "theta: tensor([0.4305, 0.5227, 0.4418, 0.5789, 0.4771, 0.4924, 0.0330],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.0484e-06.\n",
      "loss: 0.0707------------\n",
      "theta: tensor([0.4543, 0.4679, 0.4091, 0.5390, 0.4691, 0.4906, 0.0973],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 4.0484e-06.\n",
      "loss: 0.0114------------\n",
      "theta: tensor([0.4304, 0.5249, 0.4452, 0.5824, 0.4786, 0.4929, 0.0303],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.6435e-06.\n",
      "loss: 0.0823------------\n",
      "theta: tensor([0.4667, 0.4519, 0.3942, 0.5246, 0.4634, 0.4828, 0.1273],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.6435e-06.\n",
      "loss: 0.0390------------\n",
      "theta: tensor([0.4311, 0.5169, 0.4374, 0.5741, 0.4758, 0.4924, 0.0380],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.2792e-06.\n",
      "loss: 0.0105------------\n",
      "theta: tensor([0.4305, 0.5260, 0.4443, 0.5806, 0.4772, 0.4913, 0.0303],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 3.2792e-06.\n",
      "loss: 0.0216------------\n",
      "theta: tensor([0.4307, 0.5230, 0.4445, 0.5837, 0.4799, 0.4956, 0.0317],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.9513e-06.\n",
      "loss: 0.0308------------\n",
      "theta: tensor([0.4328, 0.5103, 0.4327, 0.5684, 0.4744, 0.4915, 0.0439],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.9513e-06.\n",
      "loss: 0.0188------------\n",
      "theta: tensor([0.4338, 0.5065, 0.4305, 0.5650, 0.4735, 0.4910, 0.0475],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Total number of non-zero gradients: 1\n",
      "Indices of non-zero gradients: [5, 0, 6]\n",
      "Adjusting learning rate of group 0 to 2.6561e-06.\n",
      "tensor([0.1882, 0.2556, 0.3093, 0.3400, 0.1483, 0.1847, 0.3045],\n",
      "       requires_grad=True)\n",
      "tensor([0.4284, 0.5271, 0.4467, 0.5822, 0.4775, 0.4936, 0.0298],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bucket_nn.set_value(PRECIP_RECORD, torch.tensor(network_precip_input_list, requires_grad=False))\n",
    "y_pred, loss = train_ncnn(bucket_nn, network_precip_tensor, network_outflow_tensor_0)\n",
    "network_outflow_list_1b = list(y_pred.detach().numpy())\n",
    "print(bucket_net.theta)\n",
    "print(bucket_nn.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb89e7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x333.333 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DO_PLOT = True\n",
    "if DO_PLOT:\n",
    "    startplt = 230\n",
    "    endplt = 280\n",
    "    bigfontsize = 12\n",
    "    smallfontsize = 10\n",
    "    # Set figure size to 7 inches wide\n",
    "    figure_width = 5  # inches\n",
    "    aspect_ratio = 3 / 2  # Adjust as needed for your plot\n",
    "    figure_height = figure_width / aspect_ratio\n",
    "\n",
    "    plt.figure(figsize=(figure_width, figure_height))\n",
    "\n",
    "    # Set color\n",
    "    light_blue_color = \"#3D68CA\"\n",
    "    line_and_text_color = \"#0D3F6E\"\n",
    "\n",
    "    # Plot each series with specified color\n",
    "    plt.plot(network_precip_input_list[startplt:endplt], c=light_blue_color, lw=3, label=\"Precipitation input\")\n",
    "    plt.plot(network_outflow_list_0[startplt:endplt], c=\"k\", lw=3, label=\"Target hydrograph \")\n",
    "    plt.plot(network_outflow_list_1a[startplt:endplt], \"--\", lw=2, c=line_and_text_color, label=\"Pre-trained network\")\n",
    "    plt.plot(network_outflow_list_1b[startplt:endplt], lw=3, c=\"r\", label=\"Trained bucket network\")\n",
    "\n",
    "    network_precip_tensor = torch.tensor(network_precip_input_list)\n",
    "    max_value = torch.max(network_precip_tensor[startplt:endplt]).item()\n",
    "    plt.ylim([0, max_value * 1.2])\n",
    "\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.ylabel(\"Unit hydrograph\", fontsize=bigfontsize, color=line_and_text_color)\n",
    "    plt.xlabel(\"Time\", fontsize=bigfontsize, color=line_and_text_color)\n",
    "\n",
    "    # Modify legend to have a transparent background\n",
    "    plt.legend(fontsize=smallfontsize, edgecolor=line_and_text_color, framealpha=0.5)  # Adjust framealpha as needed\n",
    "\n",
    "#    plt.title(\"Flow out from network and precip\", fontsize=bigfontsize, color=line_and_text_color)\n",
    "\n",
    "    # Save the figure with transparent background and at 300 DPI\n",
    "    plt.show()\n",
    "    #plt.savefig(\"ncn_plot.png\", transparent=True, dpi=300)\n",
    "\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d717beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'H': tensor([9.1809]),\n",
       "  'S': tensor([[[0.4963, 0.7682],\n",
       "           [0.0885, 0.1320],\n",
       "           [0.3074, 0.6341]]]),\n",
       "  's_q': tensor([[2.3641, 0.3916, 2.2064]])},\n",
       " 1: {'H': tensor([0.8840, 0.6749, 4.3277]),\n",
       "  'S': tensor([[[0.4901, 0.8964]],\n",
       "  \n",
       "          [[0.4556, 0.6323]],\n",
       "  \n",
       "          [[0.3489, 0.4017]]]),\n",
       "  's_q': tensor([[2.1538],\n",
       "          [0.2988],\n",
       "          [0.7113]])},\n",
       " 2: {'H': tensor([7.8492]),\n",
       "  'S': tensor([[[0.0223, 0.1689]]]),\n",
       "  's_q': tensor([[0.6632]])}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_net.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a91cb5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'H': tensor([15.6328], grad_fn=<ClampBackward1>),\n",
       "  'S': tensor([[[0.1760, 0.1006],\n",
       "           [0.7194, 0.2580],\n",
       "           [0.7622, 0.0443]]]),\n",
       "  's_q': tensor([[0.8230, 2.0563, 0.2851]], grad_fn=<MulBackward0>)},\n",
       " 1: {'H': tensor([8.2298e-01, 2.5820e+00, 2.9802e-08], grad_fn=<ClampBackward1>),\n",
       "  'S': tensor([[[0.8567, 0.7412]],\n",
       "  \n",
       "          [[0.5334, 0.3216]],\n",
       "  \n",
       "          [[0.3012, 0.7056]]]),\n",
       "  's_q': tensor([[0.0000],\n",
       "          [1.2309],\n",
       "          [0.5255]], grad_fn=<MulBackward0>)},\n",
       " 2: {'H': tensor([13.8270], grad_fn=<ClampBackward1>),\n",
       "  'S': tensor([[[0.4511, 0.7733]]]),\n",
       "  's_q': tensor([[0.3783]], grad_fn=<MulBackward0>)}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_nn.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "568b565c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1882, 0.2556, 0.3093, 0.3400, 0.1483, 0.1847, 0.3045],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_net.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3befa50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4284, 0.5271, 0.4467, 0.5822, 0.4775, 0.4936, 0.0298],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_nn.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d204aa2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4727, 0.5021, 0.4821, 0.5319, 0.5291, 0.5217, 0.5212])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origional_bucket_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "888d8141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.5756, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(bucket_nn.theta - origional_bucket_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "553672bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0443,  0.0249, -0.0354,  0.0503, -0.0517, -0.0281, -0.4914],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_nn.theta - origional_bucket_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'thislinewillstopthenotebookfromrunningthecellsbelow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mthislinewillstopthenotebookfromrunningthecellsbelow\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'thislinewillstopthenotebookfromrunningthecellsbelow' is not defined"
     ]
    }
   ],
   "source": [
    "print(thislinewillstopthenotebookfromrunningthecellsbelow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N_TIMESTEPS):\n",
    "\n",
    "    ###########################################################################\n",
    "    ###########################################################################\n",
    "    bucket_nn.set_value(PRECIP_SVN, torch.tensor(network_precip_input_list[i], requires_grad=True))\n",
    "    if bucket_nn.do_predict_theta_with_lstm:\n",
    "        sequence_tensors = []\n",
    "        tensor_device = bucket_nn.network[0]['H'].device\n",
    "        tensor_dtype = torch.float32\n",
    "        if i >= bucket_nn.input_u_sequence_length:\n",
    "            sequence_tensors = [torch.tensor([item], device=tensor_device, dtype=tensor_dtype) \n",
    "                                for item in network_precip_input_list[i-bucket_nn.input_u_sequence_length:i]]\n",
    "        else:\n",
    "            desired_tensor_shape = (1,)\n",
    "            padding_size = bucket_nn.input_u_sequence_length - i\n",
    "            padding_tensors = [torch.zeros(desired_tensor_shape, device=tensor_device, dtype=tensor_dtype) \n",
    "                            for _ in range(padding_size)]\n",
    "            sequence_tensors = padding_tensors + [torch.tensor([item], device=tensor_device, dtype=tensor_dtype) \n",
    "                                                for item in network_precip_input_list[:i]]\n",
    "        sequence = torch.stack(sequence_tensors).view(1, -1)\n",
    "        bucket_nn.set_value(PRECIP_SVN_SEQ, sequence)\n",
    "\n",
    "\n",
    "    bucket_nn.update()\n",
    "    network_outflow_list_1b.append(bucket_nn.network_outflow.item())\n",
    "    bucket_nn.summarize_network()\n",
    "\n",
    "    if i in [180, 200]:\n",
    "        print(bucket_nn.theta)\n",
    "        print(bucket_nn.get_the_H_tensor())\n",
    "    ###########################################################################\n",
    "    ###########################################################################\n",
    "\n",
    "    if DO_PLOT:\n",
    "        if i % int(N_TIMESTEPS/10) == 0:\n",
    "            plt.plot([tensor.item() for tensor in bucket_nn.mean_H_per_layer])\n",
    "\n",
    "###########################################################################\n",
    "network_outflow_tensor_1 = torch.tensor(network_outflow_list_1b, requires_grad=True)\n",
    "bucket_nn.report_out_mass_balance()\n",
    "\n",
    "origional_bucket_theta = copy.deepcopy(bucket_nn.theta.detach())\n",
    "\n",
    "if DO_PLOT:\n",
    "    plt.title(\"Mean head in each bucket per layer\")\n",
    "    plt.ylabel(\"Average head per layer\")\n",
    "    plt.xlabel(\"Network Layers\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "if DO_PLOT:\n",
    "    startplt = 0\n",
    "    endplt = 250\n",
    "\n",
    "    plt.plot(network_precip_input_list, c=\"grey\", lw=0.5, label=\"Precipitation input\")\n",
    "    plt.plot(network_outflow_list_1b, label=\"Flow out of network\")\n",
    "    plt.xlim([startplt, endplt])\n",
    "    plt.ylim([0, torch.max(torch.tensor(network_precip_input_list)[startplt:endplt]).item()])\n",
    "    plt.legend()\n",
    "    plt.title(\"Warmup period\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    startplt = int(len(network_outflow_list_1b)-(len(network_outflow_list_1b)/2))\n",
    "    endplt = int(len(network_outflow_list_1b))\n",
    "    plt.plot(network_precip_input_list, c=\"grey\", lw=0.5, label=\"Precipitation input\")\n",
    "    plt.plot(network_outflow_list_1b, label=\"Flow out of network\")\n",
    "    plt.xlim([startplt, endplt])\n",
    "    plt.legend()\n",
    "    plt.title(\"Flow out from network and precip\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(thiscellwillstopthenotebookbeforethepureneuralnetworksaretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.autograd import Variable \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using CUDA device: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays to PyTorch tensors\n",
    "x_input = torch.FloatTensor(network_precip_tensor.detach().numpy())\n",
    "y_target = torch.FloatTensor(network_outflow_tensor_0.detach().numpy())\n",
    "\n",
    "# Define the sequence length (you can adjust this)\n",
    "seq_length = 12\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(input_data, target_data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(input_data)-seq_length):\n",
    "        x_seq = input_data[i:i+seq_length]\n",
    "        y_seq = target_data[i+seq_length]\n",
    "        xs.append(x_seq)\n",
    "        ys.append(y_seq)\n",
    "    return torch.stack(xs), torch.stack(ys)\n",
    "\n",
    "X, y = create_sequences(x_input, y_target, seq_length)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=256, output_size=1, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers=num_layers)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq.view(len(input_seq), 1, -1))\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]\n",
    "\n",
    "model = LSTMModel()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    for seq, labels in zip(X, y):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq)\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch: {epoch} loss: {single_loss.item()}')\n",
    "\n",
    "print(f'Final loss: {single_loss.item()}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for seq in X:\n",
    "        predictions.append(model(seq).item())\n",
    "\n",
    "# Convert predictions to a numpy array for easy plotting\n",
    "predictions_np = np.array(predictions)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"LSTM Predictions vs Actual Data\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.plot(x_input.numpy()[seq_length:], label='Precipitation', alpha=0.7)\n",
    "plt.plot(y_target.numpy()[seq_length:], label='Actual Outflow', alpha=0.7)\n",
    "plt.plot(predictions_np, label='Predicted Outflow', alpha=0.7, linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays to PyTorch tensors\n",
    "x_input = torch.FloatTensor(network_precip_tensor.detach().numpy())\n",
    "y_target = torch.FloatTensor(network_outflow_tensor_0.detach().numpy())\n",
    "\n",
    "# Define the sequence length (you can adjust this)\n",
    "seq_length = 35\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(input_data, target_data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(input_data) - seq_length):\n",
    "        x_seq = input_data[i:i + seq_length]\n",
    "        y_seq = target_data[i + seq_length]\n",
    "        xs.append(x_seq)\n",
    "        ys.append(y_seq)\n",
    "    return torch.stack(xs), torch.stack(ys)\n",
    "\n",
    "X, y = create_sequences(x_input, y_target, seq_length)\n",
    "\n",
    "# Feedforward Neural Network Model\n",
    "class FFNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size=3, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        # Define your feedforward layers here\n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        self.activations = []\n",
    "        x = self.fc1(input_seq)\n",
    "        self.activations.append(x.detach().numpy())  # Record activation\n",
    "        x = self.relu1(x)\n",
    "        self.activations.append(x.detach().numpy())  # Record activation\n",
    "        x = self.fc2(x)\n",
    "        self.activations.append(x.detach().numpy())  # Record activation\n",
    "        x = self.relu2(x)\n",
    "        self.activations.append(x.detach().numpy())  # Record activation\n",
    "        x = self.fc3(x)\n",
    "        self.activations.append(x.detach().numpy())  # Record activation\n",
    "        predictions = self.fc4(x)\n",
    "        return predictions[-1]\n",
    "\n",
    "model = FFNNModel(input_size=seq_length)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.006)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.99, verbose=True)\n",
    "\n",
    "epochs = 150\n",
    "for epoch in range(epochs):\n",
    "    for seq, labels in zip(X, y):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq)\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch: {epoch} loss: {single_loss.item():.4f}')\n",
    "\n",
    "print(f'Final loss: {single_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input.shape, y_target.shape, predictions_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, seq in enumerate(X):\n",
    "        predictions.append(model(seq).item())\n",
    "predictions_np = np.array(predictions)\n",
    "\n",
    "DO_PLOT = True\n",
    "if DO_PLOT:\n",
    "    startplt = 230\n",
    "    endplt = 280\n",
    "    bigfontsize = 22\n",
    "    smallfontsize = 16\n",
    "    # Set figure size to 7 inches wide\n",
    "    figure_width = 7  # inches\n",
    "    aspect_ratio = 3 / 2  # Adjust as needed for your plot\n",
    "    figure_height = figure_width / aspect_ratio\n",
    "\n",
    "    plt.figure(figsize=(figure_width, figure_height))\n",
    "\n",
    "    # Set color\n",
    "    light_blue_color = \"#3D68CA\"\n",
    "    line_and_text_color = \"#0D3F6E\"\n",
    "\n",
    "    # Plot each series with specified color\n",
    "    plt.plot(network_precip_input_list[startplt:endplt], c=light_blue_color, lw=10, label=\"Precipitation input\")\n",
    "    plt.plot(network_outflow_list_0[startplt:endplt], c=\"k\", lw=10, label=\"Target hydrograph \")\n",
    "    # plt.plot(network_outflow_list_1a[startplt:endplt], \"--\", lw=5, c=line_and_text_color, label=\"Pre-trained network\")\n",
    "    plt.plot(predictions_np[startplt-seq_length:endplt-seq_length], \"--\", lw=10, c=line_and_text_color, label=\"Trained bucket network\")\n",
    "\n",
    "    network_precip_tensor = torch.tensor(network_precip_input_list)\n",
    "    max_value = torch.max(network_precip_tensor[startplt:endplt]).item()\n",
    "    plt.ylim([0, max_value * 1.2])\n",
    "\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.ylabel(\"Unit hydrograph\", fontsize=bigfontsize, color=line_and_text_color)\n",
    "    plt.xlabel(\"Time\", fontsize=bigfontsize, color=line_and_text_color)\n",
    "\n",
    "    # Modify legend to have a transparent background\n",
    "    #plt.legend(fontsize=smallfontsize, edgecolor=line_and_text_color, framealpha=0.5)  # Adjust framealpha as needed\n",
    "\n",
    "    #plt.title(\"Flow out from network and precip\", fontsize=bigfontsize, color=line_and_text_color)\n",
    "\n",
    "    # Save the figure with transparent background and at 300 DPI\n",
    "    #plt.show()\n",
    "    plt.savefig(\"./figs/ffn_plot.png\", transparent=True, dpi=300)\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot_the_time_series(x_input, y_target, predictions_np, seq_length):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(\"FFNN Predictions vs Actual Data\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Output\")\n",
    "    plt.plot(x_input.numpy()[seq_length:], label='Precipitation', alpha=0.7)\n",
    "    plt.plot(y_target.numpy()[seq_length:], label='Actual Outflow', alpha=0.7)\n",
    "    plt.plot(predictions_np, label='Predicted Outflow', alpha=0.7, linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, seq in enumerate(X):\n",
    "        predictions.append(model(seq).item())\n",
    "predictions_np = np.array(predictions)\n",
    "plot_the_time_series(x_input, y_target, predictions_np, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.cm import ScalarMappable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(thiscellwillstopthenotebookbeforetheanimationismade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer_connections_with_annotations(model, input_seq, title):\n",
    "    model(input_seq)  # Forward pass to record activations\n",
    "    activations = model.activations\n",
    "\n",
    "    # Concatenate activations horizontally\n",
    "    concatenated_activations = np.hstack([activation.reshape(-1, 1) for activation in activations])\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(3, 2))  # Adjust the figure size as needed\n",
    "    ax = sns.heatmap(concatenated_activations, annot=True, fmt=\".2f\", cmap=\"viridis\", annot_kws={\"size\": 8})\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Layer Neurons\")\n",
    "    plt.ylabel(\"Activation\")\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "for i in [50, 75]:\n",
    "    visualize_layer_connections_with_annotations(model, X[i], f\"Neuron Connections at x={i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_visualization_subplot(model, input_seq, layer_sizes, title, ax):\n",
    "    model(input_seq)  # Forward pass to record activations\n",
    "    activations = model.activations\n",
    "\n",
    "    # Create a color map\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "\n",
    "    # Normalize the activation values to [-5, 5] for the color map\n",
    "    norm = mcolors.Normalize(vmin=-5, vmax=5)\n",
    "\n",
    "    # Number of layers including input and output\n",
    "    num_layers = len(layer_sizes)\n",
    "\n",
    "    # List to store node positions, to be used to draw the edges\n",
    "    node_positions = {}\n",
    "\n",
    "    # Generate positions for each layer\n",
    "    for i, size in enumerate(layer_sizes):\n",
    "        # Vertical positions\n",
    "        v_positions = np.linspace(0, 1, size + 2)[1:-1]\n",
    "\n",
    "        # Horizontal position\n",
    "        h_position = i / (num_layers - 1)\n",
    "\n",
    "        # Draw nodes\n",
    "        for j, v in enumerate(v_positions):\n",
    "            activation = 0\n",
    "            if i > 0:  # Skip input layer for activations\n",
    "                activation = activations[i-1][j]\n",
    "            color = cmap(norm(activation))\n",
    "\n",
    "            circle = plt.Circle((h_position, v), 0.05, color=color, zorder=4)\n",
    "            ax.add_artist(circle)\n",
    "            node_positions[(i, j)] = (h_position, v)\n",
    "\n",
    "            # Optionally, add activation values as text inside the nodes\n",
    "            # plt.text(h_position, v, f'{activation:.2f}', ha='center', va='center', color='white', fontsize=8)\n",
    "\n",
    "    # Draw edges\n",
    "    for i in range(num_layers - 1):\n",
    "        for j in range(layer_sizes[i]):\n",
    "            for k in range(layer_sizes[i + 1]):\n",
    "                start_pos = node_positions[(i, j)]\n",
    "                end_pos = node_positions[(i + 1, k)]\n",
    "                line = plt.Line2D([start_pos[0], end_pos[0]], [start_pos[1], end_pos[1]], c='black', alpha=0.3)\n",
    "                ax.add_line(line)\n",
    "\n",
    "    mappable = ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "    # Add a colorbar\n",
    "    plt.colorbar(mappable, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Set title and turn off axis\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_time_series_subplot(x_input, y_target, predictions_np, seq_length, t, ax):\n",
    "    ax.set_title(\"FFNN Predictions vs Actual Data\")\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "    ax.set_ylabel(\"Output\")\n",
    "    ax.plot(x_input.numpy()[seq_length:], label='Precipitation', alpha=0.7)\n",
    "    ax.plot(y_target.numpy()[seq_length:], label='Actual Outflow', alpha=0.7)\n",
    "    ax.plot(predictions_np, label='Predicted Outflow', alpha=0.7, linestyle='--')\n",
    "    ax.axvline(x=t, color='red', linestyle='--')  # Vertical line\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of each layer (including input and output layers)\n",
    "layer_sizes = [1, 3, 3, 3, 1]  # Example: input layer with 35 nodes, three hidden layers with 3 nodes each, and an output layer with 1 node\n",
    "\n",
    "# Usage\n",
    "for t in range(1,200):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "    # Network visualization on the first subplot\n",
    "    network_visualization_subplot(model, X[t], layer_sizes, f\"Neural Network Visualization at t={t}\", ax1)\n",
    "\n",
    "    # Time series plot on the second subplot\n",
    "    plot_the_time_series_subplot(x_input, y_target, predictions_np, seq_length, t, ax2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./figs/network_plot/combined_frame_{t}.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [f\"./figs/network_plot/combined_frame_{t}.png\" for t in range(1, 200)]\n",
    "images = [imageio.imread(filename) for filename in filenames]\n",
    "imageio.mimsave('network_animation.gif', images, fps=2)  # Adjust fps as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
